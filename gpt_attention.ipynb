{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4435158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(embed_dim,embed_dim),\n",
    "            nn.Linear(embed_dim,embed_dim),\n",
    "            nn.Linear(embed_dim,embed_dim),\n",
    "            nn.Linear(embed_dim,vocab_size)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,seq):\n",
    "        # seq batch seq_len\n",
    "        embedding = self.embedding(seq) # batch seq_len embedding_dim\n",
    "        logits = self.seq(embedding) # batch vocab_size\n",
    "        probs = self.sigmoid(logits) # batch vocab_size\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b157f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69e08746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000,  1.0000], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln = nn.LayerNorm(2)\n",
    "ln(torch.tensor([1.0,2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfc71cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(Q,K,V):\n",
    "    # Q batch,seq_len,embedding_size\n",
    "    batch_size,seql_len,embed_size = Q.shape\n",
    "    weights = Q @ K.transpose(-1,-2) \n",
    "    mask = torch.tril(torch.ones((seql_len,seql_len))) \n",
    "    weights = torch.masked_fill(weights,mask==0.,-torch.inf)\n",
    "#     print(weights)\n",
    "    weights = F.softmax(weights,dim=-1)\n",
    "#     print(weights)\n",
    "    V = weights @ V\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42f4db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    \n",
    "    def __init__(self,q_size,k_size,v_size,num_heads,embedding_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.q_linear = nn.Linear(q_size,embedding_size//num_heads)\n",
    "        self.k_linear = nn.Linear(k_size,embedding_size//num_heads)\n",
    "        self.v_linear = nn.Linear(v_size,embedding_size//num_heads)\n",
    "    \n",
    "    \n",
    "    def forward(self,Q,K,V):\n",
    "        queries = self.q_linear(Q)\n",
    "        keys = self.k_linear(K)\n",
    "        values = self.v_linear(V)\n",
    "        ans = [attention(queries,keys,values) for _ in range(self.num_heads)]\n",
    "        return torch.cat(ans,axis=-1)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4586e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \n",
    "    def __init__(self,embed_size,hidden_size,output_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(embed_size,hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size,output_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        return self.linear2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6317cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self,q_size,k_size,v_size,num_heads,embedding_size,hidden_size):\n",
    "        super().__init__()\n",
    "        self.mh = MultiHead(q_size,k_size,v_size,num_heads,embedding_size)\n",
    "        self.ln1 = nn.LayerNorm(embedding_size)\n",
    "        self.f1 = FFN(embedding_size,hidden_size,embedding_size)\n",
    "        self.ln2 = nn.LayerNorm(embedding_size)\n",
    "    \n",
    "    def forward(self,Q,K,V):\n",
    "        t = self.mh(Q,K,V)\n",
    "        x = V + t\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.f1(x)\n",
    "        x = self.ln2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "982fd0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.ones((2,3,100))\n",
    "K = torch.ones((2,3,100))\n",
    "V = torch.ones((2,3,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9ba6ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "mh = MultiHead(Q.shape[-1],K.shape[-1],V.shape[-1],5,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0dbcd4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = Block(Q.shape[-1],K.shape[-1],V.shape[-1],5,100,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94490b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=block(Q,K,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "268c4558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 100])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35abab4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c8fd8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,q_size,k_size,v_size,num_heads,embedding_size,hidden_size,block_size,vocab_size):\n",
    "        super().__init__()\n",
    "        self.emdedding = nn.Embedding(vocab_size,embedding_size)\n",
    "        self.blks = [Block(q_size,k_size,v_size,num_heads,embedding_size,hidden_size) for _ in range(block_size)]\n",
    "        self.dense = nn.Linear(embedding_size,vocab_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.emdedding(x)\n",
    "        for blk in self.blks:\n",
    "            x = blk(x,x,x)\n",
    "        x = self.dense(x)\n",
    "        out = self.sigmoid(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed87ab11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "42d7a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(\n",
    "    [\n",
    "    [[1,2.0,3.0],\n",
    "     [3.0,2,4.0],\n",
    "     [0.1,0.8,1.0]]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55f964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0bbce550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_process import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bcb8e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx,yy=get_data(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c8d1defa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 71, 120,  24,  79,  44, 104, 111,  71,  29,  56],\n",
       "        [ 37,  36, 126,  11,   6,  45,  23, 120, 102, 131],\n",
       "        [125, 117,  96,  13,   5,  46, 124,  40,  83,  87],\n",
       "        [ 30,  91, 126,  27,  17,  63,  69, 108, 100, 101],\n",
       "        [ 90,  14,  75,   3,  70,   2,  33,  55,  77,  46],\n",
       "        [126,  28,  39,  47,  70,  10,  20,  25,  86, 121],\n",
       "        [ 52, 126,  72,  20,  70,  62,  50,  51, 127,  17],\n",
       "        [ 67, 116,  98,  68, 112, 102,  12,  37,  36,  10],\n",
       "        [ 25,  86, 121,  15, 102,  73, 120,  57,  80, 126],\n",
       "        [ 72,  20,  70,  62,  50,  51, 127,  17,  19,  67]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62de6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c13176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa352ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_size,k_size,v_size,num_heads,embedding_size,hidden_size,block_size,vocab_size\n",
    "encoder = Encoder(embedding_size,embedding_size,embedding_size,5,embedding_size,50,5,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "681bf624",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = encoder(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b69fe86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 132])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e7fc820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6240, 0.6134, 0.5480,  ..., 0.3250, 0.3431, 0.7759],\n",
       "         [0.6767, 0.6426, 0.5420,  ..., 0.4662, 0.3688, 0.7526],\n",
       "         [0.4928, 0.6028, 0.4687,  ..., 0.3853, 0.2707, 0.7767],\n",
       "         ...,\n",
       "         [0.7209, 0.6028, 0.5507,  ..., 0.3229, 0.2726, 0.7504],\n",
       "         [0.6676, 0.3732, 0.4330,  ..., 0.6668, 0.1918, 0.4468],\n",
       "         [0.6111, 0.6311, 0.4128,  ..., 0.4585, 0.3223, 0.6731]],\n",
       "\n",
       "        [[0.5660, 0.2486, 0.3755,  ..., 0.8367, 0.5912, 0.2465],\n",
       "         [0.5478, 0.4433, 0.6671,  ..., 0.7025, 0.6978, 0.2752],\n",
       "         [0.5465, 0.4497, 0.5606,  ..., 0.6658, 0.7098, 0.5451],\n",
       "         ...,\n",
       "         [0.5712, 0.4291, 0.5413,  ..., 0.5106, 0.4842, 0.5862],\n",
       "         [0.7282, 0.4962, 0.4334,  ..., 0.3872, 0.3813, 0.5260],\n",
       "         [0.5274, 0.4791, 0.6872,  ..., 0.4533, 0.4209, 0.4695]],\n",
       "\n",
       "        [[0.6622, 0.6223, 0.4643,  ..., 0.5422, 0.4607, 0.7161],\n",
       "         [0.6041, 0.6498, 0.5550,  ..., 0.4449, 0.5251, 0.7370],\n",
       "         [0.6934, 0.4515, 0.5388,  ..., 0.5969, 0.3832, 0.4993],\n",
       "         ...,\n",
       "         [0.5693, 0.6980, 0.3272,  ..., 0.4720, 0.3970, 0.5315],\n",
       "         [0.6128, 0.5861, 0.5372,  ..., 0.7172, 0.4969, 0.5603],\n",
       "         [0.6593, 0.4246, 0.4735,  ..., 0.3832, 0.2040, 0.7187]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5875, 0.3981, 0.5461,  ..., 0.5532, 0.6340, 0.3285],\n",
       "         [0.7162, 0.6459, 0.5618,  ..., 0.4297, 0.6075, 0.3778],\n",
       "         [0.5825, 0.5108, 0.4922,  ..., 0.5570, 0.4714, 0.3875],\n",
       "         ...,\n",
       "         [0.7457, 0.5726, 0.4583,  ..., 0.7711, 0.6632, 0.2586],\n",
       "         [0.6025, 0.5574, 0.6873,  ..., 0.6030, 0.6103, 0.2911],\n",
       "         [0.3790, 0.5242, 0.5104,  ..., 0.7084, 0.7218, 0.3386]],\n",
       "\n",
       "        [[0.6272, 0.3579, 0.5880,  ..., 0.4891, 0.7296, 0.2568],\n",
       "         [0.5942, 0.5886, 0.4788,  ..., 0.3788, 0.5701, 0.4561],\n",
       "         [0.5082, 0.7251, 0.4672,  ..., 0.3271, 0.5834, 0.5141],\n",
       "         ...,\n",
       "         [0.8387, 0.4405, 0.6069,  ..., 0.3259, 0.2588, 0.6807],\n",
       "         [0.4556, 0.5252, 0.3722,  ..., 0.3672, 0.4904, 0.6430],\n",
       "         [0.5568, 0.5559, 0.5482,  ..., 0.5583, 0.6071, 0.7291]],\n",
       "\n",
       "        [[0.3358, 0.3648, 0.5156,  ..., 0.8289, 0.5214, 0.4235],\n",
       "         [0.5762, 0.1818, 0.7175,  ..., 0.8007, 0.5907, 0.4507],\n",
       "         [0.4377, 0.2384, 0.6608,  ..., 0.7381, 0.6140, 0.2805],\n",
       "         ...,\n",
       "         [0.4262, 0.2136, 0.4483,  ..., 0.7095, 0.4719, 0.3138],\n",
       "         [0.5343, 0.5535, 0.4854,  ..., 0.5576, 0.4346, 0.4787],\n",
       "         [0.4743, 0.2186, 0.4392,  ..., 0.6112, 0.3039, 0.3048]]],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2069df7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a1375b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "07fff7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.8974, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(probs.view(-1,vocab_size),yy.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "97faf347",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_n = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e359977",
   "metadata": {},
   "outputs": [],
   "source": [
    "op = optim.Adam(encoder.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3f9dae3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8716, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9578, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9202, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9114, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9086, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9062, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9036, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100000\u001b[39m):\n\u001b[1;32m      2\u001b[0m     xx,yy \u001b[38;5;241m=\u001b[39m get_data(batch_n)\n\u001b[0;32m----> 3\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m  criterion(probs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,vocab_size),yy\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/edu_ktm/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[49], line 13\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memdedding(x)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblks:\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(x)\n\u001b[1;32m     15\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/edu_ktm/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[43], line 11\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, Q, K, V)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,Q,K,V):\n\u001b[0;32m---> 11\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m V \u001b[38;5;241m+\u001b[39m t\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/edu_ktm/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[41], line 15\u001b[0m, in \u001b[0;36mMultiHead.forward\u001b[0;34m(self, Q, K, V)\u001b[0m\n\u001b[1;32m     13\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_linear(K)\n\u001b[1;32m     14\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_linear(V)\n\u001b[0;32m---> 15\u001b[0m ans \u001b[38;5;241m=\u001b[39m [attention(queries,keys,values) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(ans,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[41], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_linear(K)\n\u001b[1;32m     14\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_linear(V)\n\u001b[0;32m---> 15\u001b[0m ans \u001b[38;5;241m=\u001b[39m [\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(ans,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[40], line 6\u001b[0m, in \u001b[0;36mattention\u001b[0;34m(Q, K, V)\u001b[0m\n\u001b[1;32m      4\u001b[0m     weights \u001b[38;5;241m=\u001b[39m Q \u001b[38;5;241m@\u001b[39m K\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \n\u001b[1;32m      5\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtril(torch\u001b[38;5;241m.\u001b[39mones((seql_len,seql_len))) \n\u001b[0;32m----> 6\u001b[0m     weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_fill(weights,\u001b[43mmask\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m0.\u001b[39;49m,\u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#     print(weights)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(weights,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100000):\n",
    "    xx,yy = get_data(batch_n)\n",
    "    probs = encoder(xx)\n",
    "    loss =  criterion(probs.view(-1,vocab_size),yy.view(-1))\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)\n",
    "    op.zero_grad()\n",
    "    loss.backward()\n",
    "    op.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f1d27153",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '你'\n",
    "\n",
    "xx = [char2index[x] for x in start]\n",
    "for _ in range(100):\n",
    "    input_x = torch.tensor(xx)\n",
    "    probs = encoder(input_x.view(1,-1))[:,-1,:].view(-1) # vocabsize\n",
    "    choice = torch.multinomial(probs,1)\n",
    "    xx = xx + [choice.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b09b4abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你想象的那么难，而且而且很多人会让人来说，入门是很多人会在入门深度，入门的那么难的阶段告反馈之后面段，并及时解决问题，入门是最困难的第一篇关于硕士生如果在结果导向的第一篇关于硕士生如果在结的那么难，后，\n"
     ]
    }
   ],
   "source": [
    "print(''.join([index2char[index] for index in xx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633264a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f40cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885294f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077cb9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edu_ktm",
   "language": "python",
   "name": "edu_ktm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_path = '/Users/liuchu/linear_algebra_strang/translate/中英翻译数据集/train/news-commentary-v13.zh-en.en'\n",
    "zh_path = '/Users/liuchu/linear_algebra_strang/translate/中英翻译数据集/train/news-commentary-v13.zh-en.zh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_en_sentences():\n",
    "    arr = []\n",
    "    with open(en_path,'r') as f:\n",
    "        count = 0\n",
    "        for line in f.readlines():\n",
    "            arr.append(line.strip())\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zh_sentences():\n",
    "    arr = []\n",
    "    with open(zh_path,'r') as f:\n",
    "        count = 0\n",
    "        for line in f.readlines():\n",
    "            arr.append(line.strip())\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentences = get_en_sentences()\n",
    "zh_sentences = get_zh_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 制作批处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1929 or 1989?',\n",
       " 'PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.',\n",
       " 'At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.',\n",
       " 'Today, the mood is much grimmer, with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.',\n",
       " 'The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).',\n",
       " 'Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.',\n",
       " 'For geo-strategists, however, the year that naturally comes to mind, in both politics and economics, is 1989.',\n",
       " 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       " 'Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and reassuring institution of financial capitalism.',\n",
       " 'Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose unfolding consequences will be felt for decades.']"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1929 年 还是 1989 年 ?',\n",
       " '巴黎 - 随着 经济危机 不断 加深 和 蔓延 ， 整个 世界 一直 在 寻找 历史 上 的 类似 事件 希望 有助于 我们 了解 目前 正在 发生 的 情况 。',\n",
       " '一 开始 ， 很多 人 把 这次 危机 比作 1982 年 或 1973 年 所 发生 的 情况 ， 这样 得 类比 是 令人 宽心 的 ， 因为 这 两段 时期 意味着 典型 的 周期性 衰退 。',\n",
       " '如今 人们 的 心情 却是 沉重 多 了 ， 许多 人 开始 把 这次 危机 与 1929 年 和 1931 年 相比 ， 即使 一些 国家 政府 的 表现 仍然 似乎 把视 目前 的 情况 为 是 典型 的 而 看见 的 衰退 。',\n",
       " '目前 的 趋势 是 ， 要么 是 过度 的 克制 （ 欧洲 ）， 要么 是 努力 的 扩展 （ 美国 ）。',\n",
       " '欧洲 在 避免 债务 和 捍卫 欧元 的 名义 下正 变得 谨慎 ， 而 美国 已经 在 许多 方面 行动 起来 ， 以 利用 这一 理想 的 时机 来 实行 急需 的 结构性 改革 。',\n",
       " '然而 ， 作为 地域 战略 学家 ， 无论是 从 政治 意义 还是 从 经济 意义 上 ， 让 我 自然 想到 的 年份 是 1989 年 。',\n",
       " '当然 ， 雷曼 兄弟 公司 的 倒闭 和 柏林墙 的 倒塌 没有 任何 关系 。',\n",
       " '事实上 ， 从 表面 上 看 ， 两者 似乎 是 完全 是 相反 的 ： 一个 是 象征 着 压抑 和 人为 分裂 的 柏林墙 的 倒塌 ， 而 另 一个 是 看似 坚不可摧 的 并 令人 安心 的 金融 资本主义 机构 的 倒塌 。',\n",
       " '然而 ， 和 1989 年 一样 ， 2008 - 2009 年 很 可能 也 能 被 视为 一个 划时代 的 改变 ， 其 带来 的 发人深省 的 后果 将 在 几十年 后 仍 能 让 我们 感受 得到 。']"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_sentence_with_punctuation(sentence):\n",
    "    # 使用正则表达式将句子分割成单词和标点符号\n",
    "    words = re.findall(r'\\w+|[^\\w\\s]', sentence)\n",
    "    return words\n",
    "\n",
    "# # 测试\n",
    "# sentence = \"PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.\"\n",
    "# word_list = split_sentence_with_punctuation(sentence)\n",
    "# print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_words_list = [split_sentence_with_punctuation(s) for s in zh_sentences]\n",
    "en_words_list = [split_sentence_with_punctuation(s) for s in en_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_max_len = max(len(alist) for alist in zh_words_list)\n",
    "en_mac_len = max(len(alist) for alist in en_words_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sentences = en_sentences\n",
    "target_sentences = zh_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 编码器\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "# 解码器\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = nn.functional.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=10):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS_token是起始符号的标记\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:  # EOS_token是结束符号的标记\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorFromSentence(vocab, sentence):\n",
    "    indices = [vocab[char] for char in sentence if char in vocab]  # Ensure char is in vocab\n",
    "    indices.append(vocab['<eos>'])  # Append EOS token\n",
    "    return torch.tensor(indices, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def translate_sentence(sentence, encoder, decoder, input_vocab, output_vocab):\n",
    "    input_tensor = tensorFromSentence(input_vocab, sentence)\n",
    "    translated_sentence = translate(encoder, decoder, input_tensor, input_vocab, output_vocab)\n",
    "    return translated_sentence\n",
    "\n",
    "def translate(encoder, decoder, input_tensor, input_vocab, output_vocab, max_length=100):\n",
    "    with torch.no_grad():\n",
    "        input_length = input_tensor.size(0)\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        # Forward pass through encoder\n",
    "        for ei in range(input_length):\n",
    "            _, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "        # Decoder's first input is SOS token\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_word = output_vocab[topi.item()]\n",
    "                decoded_words.append(decoded_word)\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return ' '.join(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设语料数据如下，这里我们使用了非常简单的例子\n",
    "chinese_sentences = zh_words_list[:]\n",
    "english_sentences = en_words_list[:]\n",
    "\n",
    "# 为了简化问题，我们不使用词嵌入，而是直接根据字母或汉字进行索引\n",
    "def create_dict(sentences):\n",
    "    vocab = set(w for s in sentences for w in s)\n",
    "    char_to_index = {char: i+1 for i, char in enumerate(vocab)}  # +1因为我们留出0作为padding\n",
    "    char_to_index['<pad>'] = 0\n",
    "    index_to_char = {i: char for char, i in char_to_index.items()}\n",
    "    return char_to_index, index_to_char\n",
    "\n",
    "chinese_vocab, chinese_index_vocab = create_dict(chinese_sentences)\n",
    "chinese_vocab['<sos>'] = max(chinese_vocab.values()) + 1\n",
    "chinese_index_vocab[chinese_vocab['<sos>']] = '<sos>'\n",
    "\n",
    "english_vocab, english_index_vocab = create_dict(english_sentences)\n",
    "\n",
    "# 将句子转换为索引\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab[char] for char in sentence]\n",
    "\n",
    "chinese_data = [sentence_to_indices(sentence, chinese_vocab) for sentence in chinese_sentences]\n",
    "english_data = [sentence_to_indices(sentence, english_vocab) for sentence in english_sentences]\n",
    "\n",
    "# 增加开始和结束标记\n",
    "\n",
    "SOS_token = english_vocab['<sos>'] = max(english_vocab.values()) + 1\n",
    "EOS_token = english_vocab['<eos>'] = max(english_vocab.values()) + 1\n",
    "english_index_vocab[SOS_token] = '<sos>'\n",
    "english_index_vocab[EOS_token] = '<eos>'\n",
    "english_data = [[SOS_token] + sentence + [EOS_token] for sentence in english_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63875, 63875)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_index_vocab),len(english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92865"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chinese_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据转换为torch张量\n",
    "def to_tensor(data):\n",
    "    return torch.tensor(data, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "chinese_tensors = [to_tensor(data) for data in chinese_data]\n",
    "english_tensors = [to_tensor(data) for data in english_data]\n",
    "\n",
    "# 训练参数设置\n",
    "num_epochs = 100\n",
    "\n",
    "encoder = Encoder(len(chinese_vocab), 256).to(device)\n",
    "decoder = Decoder(256, len(english_vocab)).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for ch_tensor, en_tensor in zip(chinese_tensors, english_tensors):\n",
    "        loss = train(ch_tensor, en_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        total_loss += loss\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(chinese_sentences):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1929', '年', '还是', '1989', '年', '?'],\n",
       " ['巴黎',\n",
       "  '-',\n",
       "  '随着',\n",
       "  '经济危机',\n",
       "  '不断',\n",
       "  '加深',\n",
       "  '和',\n",
       "  '蔓延',\n",
       "  '，',\n",
       "  '整个',\n",
       "  '世界',\n",
       "  '一直',\n",
       "  '在',\n",
       "  '寻找',\n",
       "  '历史',\n",
       "  '上',\n",
       "  '的',\n",
       "  '类似',\n",
       "  '事件',\n",
       "  '希望',\n",
       "  '有助于',\n",
       "  '我们',\n",
       "  '了解',\n",
       "  '目前',\n",
       "  '正在',\n",
       "  '发生',\n",
       "  '的',\n",
       "  '情况',\n",
       "  '。'],\n",
       " ['一',\n",
       "  '开始',\n",
       "  '，',\n",
       "  '很多',\n",
       "  '人',\n",
       "  '把',\n",
       "  '这次',\n",
       "  '危机',\n",
       "  '比作',\n",
       "  '1982',\n",
       "  '年',\n",
       "  '或',\n",
       "  '1973',\n",
       "  '年',\n",
       "  '所',\n",
       "  '发生',\n",
       "  '的',\n",
       "  '情况',\n",
       "  '，',\n",
       "  '这样',\n",
       "  '得',\n",
       "  '类比',\n",
       "  '是',\n",
       "  '令人',\n",
       "  '宽心',\n",
       "  '的',\n",
       "  '，',\n",
       "  '因为',\n",
       "  '这',\n",
       "  '两段',\n",
       "  '时期',\n",
       "  '意味着',\n",
       "  '典型',\n",
       "  '的',\n",
       "  '周期性',\n",
       "  '衰退',\n",
       "  '。'],\n",
       " ['如今',\n",
       "  '人们',\n",
       "  '的',\n",
       "  '心情',\n",
       "  '却是',\n",
       "  '沉重',\n",
       "  '多',\n",
       "  '了',\n",
       "  '，',\n",
       "  '许多',\n",
       "  '人',\n",
       "  '开始',\n",
       "  '把',\n",
       "  '这次',\n",
       "  '危机',\n",
       "  '与',\n",
       "  '1929',\n",
       "  '年',\n",
       "  '和',\n",
       "  '1931',\n",
       "  '年',\n",
       "  '相比',\n",
       "  '，',\n",
       "  '即使',\n",
       "  '一些',\n",
       "  '国家',\n",
       "  '政府',\n",
       "  '的',\n",
       "  '表现',\n",
       "  '仍然',\n",
       "  '似乎',\n",
       "  '把视',\n",
       "  '目前',\n",
       "  '的',\n",
       "  '情况',\n",
       "  '为',\n",
       "  '是',\n",
       "  '典型',\n",
       "  '的',\n",
       "  '而',\n",
       "  '看见',\n",
       "  '的',\n",
       "  '衰退',\n",
       "  '。'],\n",
       " ['目前',\n",
       "  '的',\n",
       "  '趋势',\n",
       "  '是',\n",
       "  '，',\n",
       "  '要么',\n",
       "  '是',\n",
       "  '过度',\n",
       "  '的',\n",
       "  '克制',\n",
       "  '（',\n",
       "  '欧洲',\n",
       "  '）',\n",
       "  '，',\n",
       "  '要么',\n",
       "  '是',\n",
       "  '努力',\n",
       "  '的',\n",
       "  '扩展',\n",
       "  '（',\n",
       "  '美国',\n",
       "  '）',\n",
       "  '。'],\n",
       " ['欧洲',\n",
       "  '在',\n",
       "  '避免',\n",
       "  '债务',\n",
       "  '和',\n",
       "  '捍卫',\n",
       "  '欧元',\n",
       "  '的',\n",
       "  '名义',\n",
       "  '下正',\n",
       "  '变得',\n",
       "  '谨慎',\n",
       "  '，',\n",
       "  '而',\n",
       "  '美国',\n",
       "  '已经',\n",
       "  '在',\n",
       "  '许多',\n",
       "  '方面',\n",
       "  '行动',\n",
       "  '起来',\n",
       "  '，',\n",
       "  '以',\n",
       "  '利用',\n",
       "  '这一',\n",
       "  '理想',\n",
       "  '的',\n",
       "  '时机',\n",
       "  '来',\n",
       "  '实行',\n",
       "  '急需',\n",
       "  '的',\n",
       "  '结构性',\n",
       "  '改革',\n",
       "  '。'],\n",
       " ['然而',\n",
       "  '，',\n",
       "  '作为',\n",
       "  '地域',\n",
       "  '战略',\n",
       "  '学家',\n",
       "  '，',\n",
       "  '无论是',\n",
       "  '从',\n",
       "  '政治',\n",
       "  '意义',\n",
       "  '还是',\n",
       "  '从',\n",
       "  '经济',\n",
       "  '意义',\n",
       "  '上',\n",
       "  '，',\n",
       "  '让',\n",
       "  '我',\n",
       "  '自然',\n",
       "  '想到',\n",
       "  '的',\n",
       "  '年份',\n",
       "  '是',\n",
       "  '1989',\n",
       "  '年',\n",
       "  '。'],\n",
       " ['当然',\n",
       "  '，',\n",
       "  '雷曼',\n",
       "  '兄弟',\n",
       "  '公司',\n",
       "  '的',\n",
       "  '倒闭',\n",
       "  '和',\n",
       "  '柏林墙',\n",
       "  '的',\n",
       "  '倒塌',\n",
       "  '没有',\n",
       "  '任何',\n",
       "  '关系',\n",
       "  '。'],\n",
       " ['事实上',\n",
       "  '，',\n",
       "  '从',\n",
       "  '表面',\n",
       "  '上',\n",
       "  '看',\n",
       "  '，',\n",
       "  '两者',\n",
       "  '似乎',\n",
       "  '是',\n",
       "  '完全',\n",
       "  '是',\n",
       "  '相反',\n",
       "  '的',\n",
       "  '：',\n",
       "  '一个',\n",
       "  '是',\n",
       "  '象征',\n",
       "  '着',\n",
       "  '压抑',\n",
       "  '和',\n",
       "  '人为',\n",
       "  '分裂',\n",
       "  '的',\n",
       "  '柏林墙',\n",
       "  '的',\n",
       "  '倒塌',\n",
       "  '，',\n",
       "  '而',\n",
       "  '另',\n",
       "  '一个',\n",
       "  '是',\n",
       "  '看似',\n",
       "  '坚不可摧',\n",
       "  '的',\n",
       "  '并',\n",
       "  '令人',\n",
       "  '安心',\n",
       "  '的',\n",
       "  '金融',\n",
       "  '资本主义',\n",
       "  '机构',\n",
       "  '的',\n",
       "  '倒塌',\n",
       "  '。'],\n",
       " ['然而',\n",
       "  '，',\n",
       "  '和',\n",
       "  '1989',\n",
       "  '年',\n",
       "  '一样',\n",
       "  '，',\n",
       "  '2008',\n",
       "  '-',\n",
       "  '2009',\n",
       "  '年',\n",
       "  '很',\n",
       "  '可能',\n",
       "  '也',\n",
       "  '能',\n",
       "  '被',\n",
       "  '视为',\n",
       "  '一个',\n",
       "  '划时代',\n",
       "  '的',\n",
       "  '改变',\n",
       "  '，',\n",
       "  '其',\n",
       "  '带来',\n",
       "  '的',\n",
       "  '发人深省',\n",
       "  '的',\n",
       "  '后果',\n",
       "  '将',\n",
       "  '在',\n",
       "  '几十年',\n",
       "  '后',\n",
       "  '仍',\n",
       "  '能',\n",
       "  '让',\n",
       "  '我们',\n",
       "  '感受',\n",
       "  '得到',\n",
       "  '。']]"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_words_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1929 or 1989?',\n",
       " 'PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.',\n",
       " 'At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.',\n",
       " 'Today, the mood is much grimmer, with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.',\n",
       " 'The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).',\n",
       " 'Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.',\n",
       " 'For geo-strategists, however, the year that naturally comes to mind, in both politics and economics, is 1989.',\n",
       " 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       " 'Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and reassuring institution of financial capitalism.',\n",
       " 'Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose unfolding consequences will be felt for decades.']"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'难': 1,\n",
       " '。': 2,\n",
       " '巴黎': 3,\n",
       " '看来': 4,\n",
       " '现在': 5,\n",
       " '脆弱': 6,\n",
       " '一个': 7,\n",
       " '压抑': 8,\n",
       " '一样': 9,\n",
       " '在': 10,\n",
       " '两段': 11,\n",
       " '整个': 12,\n",
       " '事件': 13,\n",
       " '正在': 14,\n",
       " '1973': 15,\n",
       " '类比': 16,\n",
       " '当然': 17,\n",
       " '事情': 18,\n",
       " '军备竞赛': 19,\n",
       " '政府': 20,\n",
       " '与': 21,\n",
       " '欧元': 22,\n",
       " '事实上': 23,\n",
       " '赢家': 24,\n",
       " '实行': 25,\n",
       " '一直': 26,\n",
       " '柏林墙': 27,\n",
       " '只是': 28,\n",
       " '类似': 29,\n",
       " '是': 30,\n",
       " '当今': 31,\n",
       " '世界': 32,\n",
       " '或许': 33,\n",
       " '每个': 34,\n",
       " '了': 35,\n",
       " '解体': 36,\n",
       " '积极': 37,\n",
       " '中国': 38,\n",
       " '他': 39,\n",
       " '意识形态': 40,\n",
       " '显示': 41,\n",
       " '对': 42,\n",
       " '债务': 43,\n",
       " '新': 44,\n",
       " '市场': 45,\n",
       " '战略': 46,\n",
       " '相反': 47,\n",
       " '把': 48,\n",
       " '然而': 49,\n",
       " '资本主义': 50,\n",
       " '其': 51,\n",
       " '明显': 52,\n",
       " '绝对': 53,\n",
       " '更': 54,\n",
       " '崩溃': 55,\n",
       " '改变': 56,\n",
       " '1931': 57,\n",
       " '集团': 58,\n",
       " '看': 59,\n",
       " '目前': 60,\n",
       " '了解': 61,\n",
       " '大多数': 62,\n",
       " '感受': 63,\n",
       " '似乎': 64,\n",
       " '民主': 65,\n",
       " '欧洲': 66,\n",
       " '一些': 67,\n",
       " '自由民主': 68,\n",
       " '后': 69,\n",
       " '信心': 70,\n",
       " '行动': 71,\n",
       " '周期性': 72,\n",
       " '寻找': 73,\n",
       " '沉重': 74,\n",
       " '如今': 75,\n",
       " '威胁': 76,\n",
       " '危机': 77,\n",
       " '或': 78,\n",
       " '想到': 79,\n",
       " '视为': 80,\n",
       " '来说': 81,\n",
       " '任何': 82,\n",
       " '宽心': 83,\n",
       " '完全': 84,\n",
       " '看似': 85,\n",
       " '经济危机': 86,\n",
       " '人们': 87,\n",
       " '让': 88,\n",
       " '重要': 89,\n",
       " '意义': 90,\n",
       " '1982': 91,\n",
       " '困境': 92,\n",
       " '支持者': 93,\n",
       " '将': 94,\n",
       " '后果': 95,\n",
       " '预期': 96,\n",
       " '更大': 97,\n",
       " '发人深省': 98,\n",
       " '已经': 99,\n",
       " '）': 100,\n",
       " '能': 101,\n",
       " '东西方': 102,\n",
       " '历史': 103,\n",
       " '从': 104,\n",
       " '充分': 105,\n",
       " '责任': 106,\n",
       " '发生': 107,\n",
       " '作为': 108,\n",
       " '比作': 109,\n",
       " '与此相反': 110,\n",
       " '国家': 111,\n",
       " '区分': 112,\n",
       " '但': 113,\n",
       " '为': 114,\n",
       " '学家': 115,\n",
       " '被': 116,\n",
       " '社会主义': 117,\n",
       " '却': 118,\n",
       " '情况': 119,\n",
       " '确实': 120,\n",
       " '扩展': 121,\n",
       " '努力': 122,\n",
       " '输家': 123,\n",
       " '政治': 124,\n",
       " '2009': 125,\n",
       " '民族主义': 126,\n",
       " '两极化': 127,\n",
       " '经济': 128,\n",
       " '很': 129,\n",
       " '首先': 130,\n",
       " '急需': 131,\n",
       " '最': 132,\n",
       " '推向': 133,\n",
       " '影响': 134,\n",
       " '1929': 135,\n",
       " '一': 136,\n",
       " '形式': 137,\n",
       " '典型': 138,\n",
       " '改革': 139,\n",
       " '变得': 140,\n",
       " '精心策划': 141,\n",
       " '其它': 142,\n",
       " '大': 143,\n",
       " '的': 144,\n",
       " '表面': 145,\n",
       " '希望': 146,\n",
       " '就是': 147,\n",
       " '尽管': 148,\n",
       " '几十年': 149,\n",
       " '要': 150,\n",
       " '比': 151,\n",
       " '许多': 152,\n",
       " '升级': 153,\n",
       " '随着': 154,\n",
       " '（': 155,\n",
       " '倒塌': 156,\n",
       " '1989': 157,\n",
       " '这样': 158,\n",
       " '不同': 159,\n",
       " '总统': 160,\n",
       " '开始': 161,\n",
       " '衰退': 162,\n",
       " '起来': 163,\n",
       " '人为': 164,\n",
       " '不': 165,\n",
       " '意味着': 166,\n",
       " '当时': 167,\n",
       " '走出': 168,\n",
       " '利用': 169,\n",
       " '随后': 170,\n",
       " '以及': 171,\n",
       " '倾向': 172,\n",
       " '克制': 173,\n",
       " '铺平道路': 174,\n",
       " '具体化': 175,\n",
       " '公司': 176,\n",
       " '势态': 177,\n",
       " '过度': 178,\n",
       " '无论是': 179,\n",
       " '2008': 180,\n",
       " '推崇': 181,\n",
       " '和平统一': 182,\n",
       " '地域': 183,\n",
       " '相比': 184,\n",
       " '自然': 185,\n",
       " '我们': 186,\n",
       " '另': 187,\n",
       " '会为': 188,\n",
       " '仍然': 189,\n",
       " '得到': 190,\n",
       " '金融': 191,\n",
       " '由': 192,\n",
       " '关系': 193,\n",
       " '有助于': 194,\n",
       " '公平': 195,\n",
       " '多': 196,\n",
       " '得': 197,\n",
       " '都': 198,\n",
       " '不断': 199,\n",
       " '着': 200,\n",
       " '这次': 201,\n",
       " '恐外': 202,\n",
       " '从而': 203,\n",
       " '一种': 204,\n",
       " '边缘': 205,\n",
       " '革命': 206,\n",
       " '没有': 207,\n",
       " '也许': 208,\n",
       " '以': 209,\n",
       " '，': 210,\n",
       " '并': 211,\n",
       " '转折点': 212,\n",
       " '时机': 213,\n",
       " '看见': 214,\n",
       " '人': 215,\n",
       " '心情': 216,\n",
       " '里根': 217,\n",
       " '；': 218,\n",
       " '方面': 219,\n",
       " '有些': 220,\n",
       " '划时代': 221,\n",
       " '负': 222,\n",
       " '坚不可摧': 223,\n",
       " '年': 224,\n",
       " '优越性': 225,\n",
       " '雷曼': 226,\n",
       " '所': 227,\n",
       " '理想': 228,\n",
       " '制度': 229,\n",
       " '时期': 230,\n",
       " '-': 231,\n",
       " '结束': 232,\n",
       " '年份': 233,\n",
       " '结构性': 234,\n",
       " '象征': 235,\n",
       " '蔓延': 236,\n",
       " '可能': 237,\n",
       " '社会': 238,\n",
       " '对于': 239,\n",
       " '全球': 240,\n",
       " '会': 241,\n",
       " '把视': 242,\n",
       " '这一': 243,\n",
       " '如果': 244,\n",
       " '却是': 245,\n",
       " '捍卫': 246,\n",
       " '还是': 247,\n",
       " '分裂': 248,\n",
       " '而': 249,\n",
       " '其二': 250,\n",
       " '加深': 251,\n",
       " '即使': 252,\n",
       " '名义': 253,\n",
       " '我': 254,\n",
       " '兄弟': 255,\n",
       " '成果': 256,\n",
       " '机构': 257,\n",
       " '受到': 258,\n",
       " '带来': 259,\n",
       " '要么': 260,\n",
       " '些': 261,\n",
       " '这': 262,\n",
       " '下正': 263,\n",
       " '来': 264,\n",
       " '安心': 265,\n",
       " '仍': 266,\n",
       " '战胜': 267,\n",
       " '不是': 268,\n",
       " '取代': 269,\n",
       " '两者': 270,\n",
       " '?': 271,\n",
       " '趋势': 272,\n",
       " '包括': 273,\n",
       " '自由': 274,\n",
       " '鸿沟': 275,\n",
       " '苏联': 276,\n",
       " '表现': 277,\n",
       " '倒闭': 278,\n",
       " '也': 279,\n",
       " '良好': 280,\n",
       " '因为': 281,\n",
       " '美国': 282,\n",
       " '令人': 283,\n",
       " '上': 284,\n",
       " '自由市场': 285,\n",
       " '的话': 286,\n",
       " '：': 287,\n",
       " '很多': 288,\n",
       " '谨慎': 289,\n",
       " '和': 290,\n",
       " '避免': 291,\n",
       " '<pad>': 0,\n",
       " '<sos>': 292}"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['巴黎', '-', '随着', '经济危机', '不断']"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_words_list[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> At the start of the crisis , many people likened it to 1982 or 1973 , which was reassuring , because both dates refer to classical cyclical downturns . <EOS>\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(sentence):\n",
    "    indices = sentence_to_indices(sentence, chinese_vocab)\n",
    "    tensor = to_tensor(indices)\n",
    "    translation = translate(encoder, decoder, tensor, chinese_index_vocab, english_index_vocab)\n",
    "    return translation\n",
    "\n",
    "# 翻译示例\n",
    "print(translate_sentence(zh_words_list[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch size 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 编码器\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)  # Set batch_first to True\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)  # input: (batch_size, seq_len)\n",
    "        output, hidden = self.gru(embedded, hidden)  # No need to permute\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "# 解码器\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)  # Set batch_first to True\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input)  # input: (batch_size,seq)\n",
    "#         output = output.unsqueeze(1)  # Change to (batch_size, 1, hidden_size)\n",
    "        output = nn.functional.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)  # No need to permute\n",
    "        output = self.softmax(self.out(output.squeeze(1)))  # Squeeze to remove the sequence length dimension\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=10):\n",
    "    batch_size = input_tensor.size(0)\n",
    "    input_length = input_tensor.size(1)\n",
    "    target_length = target_tensor.size(1)\n",
    "\n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # Encode the input sequence batch\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "\n",
    "    # Prepare the initial decoder input (start with SOS tokens for each sequence in the batch)\n",
    "    decoder_input = torch.tensor([[SOS_token] * batch_size], device=device).transpose(0, 1)  # Shape: (batch_size, 1)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Assuming teacher forcing is not used here. If desired, it can be included with a certain probability.\n",
    "    for di in range(target_length):\n",
    "#         print(decoder_input.shape)\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.detach()  # Prepare next input\n",
    "#         print(decoder_output.shape,target_tensor[:, di].shape,decoder_input.shape)\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[:, di])\n",
    "\n",
    "        # Optionally, stop when all sequences in the batch reach EOS token\n",
    "        if (decoder_input == EOS_token).all():\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / (target_length * batch_size)  # Normalize loss by total number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 10  # Example vocabulary size for encoder\n",
    "output_size = 10  # Example vocabulary size for decoder\n",
    "hidden_size = 256\n",
    "batch_size = 5\n",
    "max_length = 7  # Maximum length of input and output sequences\n",
    "epochs = 10\n",
    "\n",
    "# Special tokens\n",
    "SOS_token = 0  # Start-of-sequence token\n",
    "EOS_token = 1  # End-of-sequence token\n",
    "\n",
    "# Instantiate models\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(hidden_size, output_size).to(device)\n",
    "\n",
    "# Optimizers\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.01)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.01)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_batch(batch_size, max_length, vocabulary_size):\n",
    "    \"\"\" Generate a random batch of sequences for training. \"\"\"\n",
    "    # Random sequences of random lengths\n",
    "    sequences = torch.randint(2, vocabulary_size, (batch_size, max_length)).to(device)\n",
    "    return sequences\n",
    "\n",
    "# Example of generating a batch\n",
    "input_tensor = generate_fake_batch(batch_size, max_length, input_size)\n",
    "target_tensor = generate_fake_batch(batch_size, max_length, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 4, 9, 8, 2, 7],\n",
       "        [6, 6, 7, 6, 8, 2, 5],\n",
       "        [9, 5, 6, 7, 3, 4, 4],\n",
       "        [4, 5, 5, 5, 7, 7, 5],\n",
       "        [3, 5, 6, 2, 5, 2, 2]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4680\n",
      "Epoch 2, Loss: 0.5614\n",
      "Epoch 3, Loss: 0.4901\n",
      "Epoch 4, Loss: 0.5791\n",
      "Epoch 5, Loss: 0.5343\n",
      "Epoch 6, Loss: 0.4939\n",
      "Epoch 7, Loss: 0.4654\n",
      "Epoch 8, Loss: 0.4840\n",
      "Epoch 9, Loss: 0.4593\n",
      "Epoch 10, Loss: 0.4287\n",
      "Average Loss: 0.4964\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(encoder, decoder, input_tensor, target_tensor):\n",
    "    loss_total = 0  # Track loss\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Generate new data for each epoch\n",
    "        input_tensor = generate_fake_batch(batch_size, max_length, input_size)\n",
    "        target_tensor = generate_fake_batch(batch_size, max_length, output_size)\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\n",
    "        loss_total += loss\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss:.4f}')\n",
    "\n",
    "    print(f'Average Loss: {loss_total / epochs:.4f}')\n",
    "\n",
    "# Call the training function\n",
    "train_epoch(encoder, decoder, input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch_len 版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden, input_lengths):\n",
    "        embedded = self.embedding(input)  # input: (batch_size, seq_len)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, hidden = self.gru(packed, hidden)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input)  # input: (batch_size, 1)\n",
    "        output = nn.functional.relu(output)\n",
    "#         print('input shape, output shape 111',input.shape,output.shape)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "#         print('input shape, output shape  222',input.shape,output.shape)\n",
    "        output = self.softmax(self.out(output.squeeze(1)))  # Make sure output is squeezed\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, input_lengths, output_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=10):\n",
    "    batch_size = input_tensor.size(0)\n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # 编码器处理输入\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden, input_lengths)\n",
    "\n",
    "    # 解码器的初始输入\n",
    "    decoder_input = torch.tensor([[SOS_token] * batch_size], device=device).transpose(0, 1)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # 解码器生成输出\n",
    "    for di in range(max(output_lengths)):  # 使用最大输出长度\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        decoder_input = decoder_output.topk(1)[1].detach()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            if di < output_lengths[i]:  # 只计算有效长度内的损失\n",
    "                loss += criterion(decoder_output[i], target_tensor[i, di])\n",
    "\n",
    "        if (decoder_input == EOS_token).all():\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / sum(output_lengths)  # 按有效输出长度归一化损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 假设的词汇大小和隐藏层大小\n",
    "input_size = 10\n",
    "output_size = 10\n",
    "hidden_size = 64\n",
    "\n",
    "# 特殊符号定义\n",
    "SOS_token = 0  # 句子开始符号\n",
    "EOS_token = 1  # 句子结束符号\n",
    "\n",
    "# 实例化模型\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(hidden_size, output_size).to(device)\n",
    "\n",
    "# 优化器\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.01)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.01)\n",
    "\n",
    "# 损失函数\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# 创建一些虚拟数据\n",
    "def generate_batch(batch_size):\n",
    "    max_length = 10\n",
    "    input_tensor = torch.LongTensor(batch_size, max_length).random_(2, input_size).to(device)\n",
    "    target_tensor = torch.LongTensor(batch_size, max_length).random_(2, output_size).to(device)\n",
    "    input_lengths = torch.LongTensor([random.randint(5, max_length) for _ in range(batch_size)]).to(device)\n",
    "    target_lengths = torch.LongTensor([random.randint(5, max_length) for _ in range(batch_size)]).to(device)\n",
    "    return input_tensor, target_tensor, input_lengths, target_lengths\n",
    "\n",
    "batch_size = 5\n",
    "input_tensor, target_tensor, input_lengths, target_lengths = generate_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.2624869346618652\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "loss = train(input_tensor, target_tensor, input_lengths, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

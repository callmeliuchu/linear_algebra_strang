{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_path = '/Users/liuchu/linear_algebra_strang/translate/中英翻译数据集/train/news-commentary-v13.zh-en.en'\n",
    "zh_path = '/Users/liuchu/linear_algebra_strang/translate/中英翻译数据集/train/news-commentary-v13.zh-en.zh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_en_sentences():\n",
    "    arr = []\n",
    "    with open(en_path,'r') as f:\n",
    "        count = 0\n",
    "        for line in f.readlines():\n",
    "            arr.append(line.strip())\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zh_sentences():\n",
    "    arr = []\n",
    "    with open(zh_path,'r') as f:\n",
    "        count = 0\n",
    "        for line in f.readlines():\n",
    "            arr.append(line.strip())\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentences = get_en_sentences()\n",
    "zh_sentences = get_zh_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 制作批处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1929 or 1989?',\n",
       " 'PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.',\n",
       " 'At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.',\n",
       " 'Today, the mood is much grimmer, with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.',\n",
       " 'The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).',\n",
       " 'Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.',\n",
       " 'For geo-strategists, however, the year that naturally comes to mind, in both politics and economics, is 1989.',\n",
       " 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       " 'Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and reassuring institution of financial capitalism.',\n",
       " 'Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose unfolding consequences will be felt for decades.']"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1929 年 还是 1989 年 ?',\n",
       " '巴黎 - 随着 经济危机 不断 加深 和 蔓延 ， 整个 世界 一直 在 寻找 历史 上 的 类似 事件 希望 有助于 我们 了解 目前 正在 发生 的 情况 。',\n",
       " '一 开始 ， 很多 人 把 这次 危机 比作 1982 年 或 1973 年 所 发生 的 情况 ， 这样 得 类比 是 令人 宽心 的 ， 因为 这 两段 时期 意味着 典型 的 周期性 衰退 。',\n",
       " '如今 人们 的 心情 却是 沉重 多 了 ， 许多 人 开始 把 这次 危机 与 1929 年 和 1931 年 相比 ， 即使 一些 国家 政府 的 表现 仍然 似乎 把视 目前 的 情况 为 是 典型 的 而 看见 的 衰退 。',\n",
       " '目前 的 趋势 是 ， 要么 是 过度 的 克制 （ 欧洲 ）， 要么 是 努力 的 扩展 （ 美国 ）。',\n",
       " '欧洲 在 避免 债务 和 捍卫 欧元 的 名义 下正 变得 谨慎 ， 而 美国 已经 在 许多 方面 行动 起来 ， 以 利用 这一 理想 的 时机 来 实行 急需 的 结构性 改革 。',\n",
       " '然而 ， 作为 地域 战略 学家 ， 无论是 从 政治 意义 还是 从 经济 意义 上 ， 让 我 自然 想到 的 年份 是 1989 年 。',\n",
       " '当然 ， 雷曼 兄弟 公司 的 倒闭 和 柏林墙 的 倒塌 没有 任何 关系 。',\n",
       " '事实上 ， 从 表面 上 看 ， 两者 似乎 是 完全 是 相反 的 ： 一个 是 象征 着 压抑 和 人为 分裂 的 柏林墙 的 倒塌 ， 而 另 一个 是 看似 坚不可摧 的 并 令人 安心 的 金融 资本主义 机构 的 倒塌 。',\n",
       " '然而 ， 和 1989 年 一样 ， 2008 - 2009 年 很 可能 也 能 被 视为 一个 划时代 的 改变 ， 其 带来 的 发人深省 的 后果 将 在 几十年 后 仍 能 让 我们 感受 得到 。']"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_sentence_with_punctuation(sentence):\n",
    "    # 使用正则表达式将句子分割成单词和标点符号\n",
    "    words = re.findall(r'\\w+|[^\\w\\s]', sentence)\n",
    "    return words\n",
    "\n",
    "# # 测试\n",
    "# sentence = \"PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.\"\n",
    "# word_list = split_sentence_with_punctuation(sentence)\n",
    "# print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_words_list = [split_sentence_with_punctuation(s) for s in zh_sentences]\n",
    "en_words_list = [split_sentence_with_punctuation(s) for s in en_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_max_len = max(len(alist) for alist in zh_words_list)\n",
    "en_mac_len = max(len(alist) for alist in en_words_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencesTool:\n",
    "    \n",
    "    def __init__(self,sentences):\n",
    "        self.words_list = [split_sentence_with_punctuation(s) for s in sentences]\n",
    "        self.max_len = max(len(alist) for alist in self.words_list) + 1\n",
    "        self.vocab = ['<pad>','<sos>','<eos>'] + list(set(word for words in self.words_list for word in words))\n",
    "        self.index2word = {i:word for i,word in enumerate(self.vocab)}\n",
    "        self.word2index = {word:i for i,word in self.index2word.items()} \n",
    "        self.tensors = self.totensor()\n",
    "    \n",
    "    def totensor(self):\n",
    "        ans = []\n",
    "        batch_len = []\n",
    "        for words in self.words_list:\n",
    "            indexs = [self.word2index[word] for word in words +['<eos>']+ ['<pad>']*(self.max_len-len(words)-1)]\n",
    "            ans.append(indexs)\n",
    "            batch_len.append(len(words)+1)\n",
    "        return torch.tensor(ans, dtype=torch.long, device=device),torch.tensor(batch_len, dtype=torch.long, device=device)\n",
    "    \n",
    "    def toindex(self,word):\n",
    "        return self.word2index[word]\n",
    "    \n",
    "    def toword(self,index):\n",
    "        return self.index2word[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = SentencesTool(zh_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[24066,  2140, 88265,  ...,     0,     0,     0],\n",
       "         [62242, 53689, 66284,  ...,     0,     0,     0],\n",
       "         [73628,  5697, 92103,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [ 9636, 65812, 75159,  ...,     0,     0,     0],\n",
       "         [ 1322, 81455, 86240,  ...,     0,     0,     0],\n",
       "         [39343, 92103, 17799,  ...,     0,     0,     0]]),\n",
       " tensor([ 7, 30, 38,  ..., 29, 24, 42]))"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sentences = en_sentences\n",
    "target_sentences = zh_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 编码器\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "# 解码器\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = nn.functional.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=10):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS_token是起始符号的标记\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:  # EOS_token是结束符号的标记\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorFromSentence(vocab, sentence):\n",
    "    indices = [vocab[char] for char in sentence if char in vocab]  # Ensure char is in vocab\n",
    "    indices.append(vocab['<eos>'])  # Append EOS token\n",
    "    return torch.tensor(indices, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def translate_sentence(sentence, encoder, decoder, input_vocab, output_vocab):\n",
    "    input_tensor = tensorFromSentence(input_vocab, sentence)\n",
    "    translated_sentence = translate(encoder, decoder, input_tensor, input_vocab, output_vocab)\n",
    "    return translated_sentence\n",
    "\n",
    "def translate(encoder, decoder, input_tensor, input_vocab, output_vocab, max_length=100):\n",
    "    with torch.no_grad():\n",
    "        input_length = input_tensor.size(0)\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        # Forward pass through encoder\n",
    "        for ei in range(input_length):\n",
    "            _, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "        # Decoder's first input is SOS token\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_word = output_vocab[topi.item()]\n",
    "                decoded_words.append(decoded_word)\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return ' '.join(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设语料数据如下，这里我们使用了非常简单的例子\n",
    "chinese_sentences = zh_words_list[:]\n",
    "english_sentences = en_words_list[:]\n",
    "\n",
    "# 为了简化问题，我们不使用词嵌入，而是直接根据字母或汉字进行索引\n",
    "def create_dict(sentences):\n",
    "    vocab = set(w for s in sentences for w in s)\n",
    "    char_to_index = {char: i+1 for i, char in enumerate(vocab)}  # +1因为我们留出0作为padding\n",
    "    char_to_index['<pad>'] = 0\n",
    "    index_to_char = {i: char for char, i in char_to_index.items()}\n",
    "    return char_to_index, index_to_char\n",
    "\n",
    "chinese_vocab, chinese_index_vocab = create_dict(chinese_sentences)\n",
    "chinese_vocab['<sos>'] = max(chinese_vocab.values()) + 1\n",
    "chinese_index_vocab[chinese_vocab['<sos>']] = '<sos>'\n",
    "\n",
    "english_vocab, english_index_vocab = create_dict(english_sentences)\n",
    "\n",
    "# 将句子转换为索引\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab[char] for char in sentence]\n",
    "\n",
    "chinese_data = [sentence_to_indices(sentence, chinese_vocab) for sentence in chinese_sentences]\n",
    "english_data = [sentence_to_indices(sentence, english_vocab) for sentence in english_sentences]\n",
    "\n",
    "# 增加开始和结束标记\n",
    "\n",
    "SOS_token = english_vocab['<sos>'] = max(english_vocab.values()) + 1\n",
    "EOS_token = english_vocab['<eos>'] = max(english_vocab.values()) + 1\n",
    "english_index_vocab[SOS_token] = '<sos>'\n",
    "english_index_vocab[EOS_token] = '<eos>'\n",
    "english_data = [[SOS_token] + sentence + [EOS_token] for sentence in english_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63875, 63875)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_index_vocab),len(english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92865"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chinese_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据转换为torch张量\n",
    "def to_tensor(data):\n",
    "    return torch.tensor(data, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "chinese_tensors = [to_tensor(data) for data in chinese_data]\n",
    "english_tensors = [to_tensor(data) for data in english_data]\n",
    "\n",
    "# 训练参数设置\n",
    "num_epochs = 100\n",
    "\n",
    "encoder = Encoder(len(chinese_vocab), 256).to(device)\n",
    "decoder = Decoder(256, len(english_vocab)).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-422-ca9cbf82f3e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mch_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchinese_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-392-ed0640575a3f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for ch_tensor, en_tensor in zip(chinese_tensors, english_tensors):\n",
    "        loss = train(ch_tensor, en_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        total_loss += loss\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(chinese_sentences):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1929', '年', '还是', '1989', '年', '?'],\n",
       " ['巴黎',\n",
       "  '-',\n",
       "  '随着',\n",
       "  '经济危机',\n",
       "  '不断',\n",
       "  '加深',\n",
       "  '和',\n",
       "  '蔓延',\n",
       "  '，',\n",
       "  '整个',\n",
       "  '世界',\n",
       "  '一直',\n",
       "  '在',\n",
       "  '寻找',\n",
       "  '历史',\n",
       "  '上',\n",
       "  '的',\n",
       "  '类似',\n",
       "  '事件',\n",
       "  '希望',\n",
       "  '有助于',\n",
       "  '我们',\n",
       "  '了解',\n",
       "  '目前',\n",
       "  '正在',\n",
       "  '发生',\n",
       "  '的',\n",
       "  '情况',\n",
       "  '。'],\n",
       " ['一',\n",
       "  '开始',\n",
       "  '，',\n",
       "  '很多',\n",
       "  '人',\n",
       "  '把',\n",
       "  '这次',\n",
       "  '危机',\n",
       "  '比作',\n",
       "  '1982',\n",
       "  '年',\n",
       "  '或',\n",
       "  '1973',\n",
       "  '年',\n",
       "  '所',\n",
       "  '发生',\n",
       "  '的',\n",
       "  '情况',\n",
       "  '，',\n",
       "  '这样',\n",
       "  '得',\n",
       "  '类比',\n",
       "  '是',\n",
       "  '令人',\n",
       "  '宽心',\n",
       "  '的',\n",
       "  '，',\n",
       "  '因为',\n",
       "  '这',\n",
       "  '两段',\n",
       "  '时期',\n",
       "  '意味着',\n",
       "  '典型',\n",
       "  '的',\n",
       "  '周期性',\n",
       "  '衰退',\n",
       "  '。'],\n",
       " ['如今',\n",
       "  '人们',\n",
       "  '的',\n",
       "  '心情',\n",
       "  '却是',\n",
       "  '沉重',\n",
       "  '多',\n",
       "  '了',\n",
       "  '，',\n",
       "  '许多',\n",
       "  '人',\n",
       "  '开始',\n",
       "  '把',\n",
       "  '这次',\n",
       "  '危机',\n",
       "  '与',\n",
       "  '1929',\n",
       "  '年',\n",
       "  '和',\n",
       "  '1931',\n",
       "  '年',\n",
       "  '相比',\n",
       "  '，',\n",
       "  '即使',\n",
       "  '一些',\n",
       "  '国家',\n",
       "  '政府',\n",
       "  '的',\n",
       "  '表现',\n",
       "  '仍然',\n",
       "  '似乎',\n",
       "  '把视',\n",
       "  '目前',\n",
       "  '的',\n",
       "  '情况',\n",
       "  '为',\n",
       "  '是',\n",
       "  '典型',\n",
       "  '的',\n",
       "  '而',\n",
       "  '看见',\n",
       "  '的',\n",
       "  '衰退',\n",
       "  '。'],\n",
       " ['目前',\n",
       "  '的',\n",
       "  '趋势',\n",
       "  '是',\n",
       "  '，',\n",
       "  '要么',\n",
       "  '是',\n",
       "  '过度',\n",
       "  '的',\n",
       "  '克制',\n",
       "  '（',\n",
       "  '欧洲',\n",
       "  '）',\n",
       "  '，',\n",
       "  '要么',\n",
       "  '是',\n",
       "  '努力',\n",
       "  '的',\n",
       "  '扩展',\n",
       "  '（',\n",
       "  '美国',\n",
       "  '）',\n",
       "  '。'],\n",
       " ['欧洲',\n",
       "  '在',\n",
       "  '避免',\n",
       "  '债务',\n",
       "  '和',\n",
       "  '捍卫',\n",
       "  '欧元',\n",
       "  '的',\n",
       "  '名义',\n",
       "  '下正',\n",
       "  '变得',\n",
       "  '谨慎',\n",
       "  '，',\n",
       "  '而',\n",
       "  '美国',\n",
       "  '已经',\n",
       "  '在',\n",
       "  '许多',\n",
       "  '方面',\n",
       "  '行动',\n",
       "  '起来',\n",
       "  '，',\n",
       "  '以',\n",
       "  '利用',\n",
       "  '这一',\n",
       "  '理想',\n",
       "  '的',\n",
       "  '时机',\n",
       "  '来',\n",
       "  '实行',\n",
       "  '急需',\n",
       "  '的',\n",
       "  '结构性',\n",
       "  '改革',\n",
       "  '。'],\n",
       " ['然而',\n",
       "  '，',\n",
       "  '作为',\n",
       "  '地域',\n",
       "  '战略',\n",
       "  '学家',\n",
       "  '，',\n",
       "  '无论是',\n",
       "  '从',\n",
       "  '政治',\n",
       "  '意义',\n",
       "  '还是',\n",
       "  '从',\n",
       "  '经济',\n",
       "  '意义',\n",
       "  '上',\n",
       "  '，',\n",
       "  '让',\n",
       "  '我',\n",
       "  '自然',\n",
       "  '想到',\n",
       "  '的',\n",
       "  '年份',\n",
       "  '是',\n",
       "  '1989',\n",
       "  '年',\n",
       "  '。'],\n",
       " ['当然',\n",
       "  '，',\n",
       "  '雷曼',\n",
       "  '兄弟',\n",
       "  '公司',\n",
       "  '的',\n",
       "  '倒闭',\n",
       "  '和',\n",
       "  '柏林墙',\n",
       "  '的',\n",
       "  '倒塌',\n",
       "  '没有',\n",
       "  '任何',\n",
       "  '关系',\n",
       "  '。'],\n",
       " ['事实上',\n",
       "  '，',\n",
       "  '从',\n",
       "  '表面',\n",
       "  '上',\n",
       "  '看',\n",
       "  '，',\n",
       "  '两者',\n",
       "  '似乎',\n",
       "  '是',\n",
       "  '完全',\n",
       "  '是',\n",
       "  '相反',\n",
       "  '的',\n",
       "  '：',\n",
       "  '一个',\n",
       "  '是',\n",
       "  '象征',\n",
       "  '着',\n",
       "  '压抑',\n",
       "  '和',\n",
       "  '人为',\n",
       "  '分裂',\n",
       "  '的',\n",
       "  '柏林墙',\n",
       "  '的',\n",
       "  '倒塌',\n",
       "  '，',\n",
       "  '而',\n",
       "  '另',\n",
       "  '一个',\n",
       "  '是',\n",
       "  '看似',\n",
       "  '坚不可摧',\n",
       "  '的',\n",
       "  '并',\n",
       "  '令人',\n",
       "  '安心',\n",
       "  '的',\n",
       "  '金融',\n",
       "  '资本主义',\n",
       "  '机构',\n",
       "  '的',\n",
       "  '倒塌',\n",
       "  '。'],\n",
       " ['然而',\n",
       "  '，',\n",
       "  '和',\n",
       "  '1989',\n",
       "  '年',\n",
       "  '一样',\n",
       "  '，',\n",
       "  '2008',\n",
       "  '-',\n",
       "  '2009',\n",
       "  '年',\n",
       "  '很',\n",
       "  '可能',\n",
       "  '也',\n",
       "  '能',\n",
       "  '被',\n",
       "  '视为',\n",
       "  '一个',\n",
       "  '划时代',\n",
       "  '的',\n",
       "  '改变',\n",
       "  '，',\n",
       "  '其',\n",
       "  '带来',\n",
       "  '的',\n",
       "  '发人深省',\n",
       "  '的',\n",
       "  '后果',\n",
       "  '将',\n",
       "  '在',\n",
       "  '几十年',\n",
       "  '后',\n",
       "  '仍',\n",
       "  '能',\n",
       "  '让',\n",
       "  '我们',\n",
       "  '感受',\n",
       "  '得到',\n",
       "  '。']]"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_words_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1929 or 1989?',\n",
       " 'PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.',\n",
       " 'At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.',\n",
       " 'Today, the mood is much grimmer, with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.',\n",
       " 'The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).',\n",
       " 'Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.',\n",
       " 'For geo-strategists, however, the year that naturally comes to mind, in both politics and economics, is 1989.',\n",
       " 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       " 'Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and reassuring institution of financial capitalism.',\n",
       " 'Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose unfolding consequences will be felt for decades.']"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'难': 1,\n",
       " '。': 2,\n",
       " '巴黎': 3,\n",
       " '看来': 4,\n",
       " '现在': 5,\n",
       " '脆弱': 6,\n",
       " '一个': 7,\n",
       " '压抑': 8,\n",
       " '一样': 9,\n",
       " '在': 10,\n",
       " '两段': 11,\n",
       " '整个': 12,\n",
       " '事件': 13,\n",
       " '正在': 14,\n",
       " '1973': 15,\n",
       " '类比': 16,\n",
       " '当然': 17,\n",
       " '事情': 18,\n",
       " '军备竞赛': 19,\n",
       " '政府': 20,\n",
       " '与': 21,\n",
       " '欧元': 22,\n",
       " '事实上': 23,\n",
       " '赢家': 24,\n",
       " '实行': 25,\n",
       " '一直': 26,\n",
       " '柏林墙': 27,\n",
       " '只是': 28,\n",
       " '类似': 29,\n",
       " '是': 30,\n",
       " '当今': 31,\n",
       " '世界': 32,\n",
       " '或许': 33,\n",
       " '每个': 34,\n",
       " '了': 35,\n",
       " '解体': 36,\n",
       " '积极': 37,\n",
       " '中国': 38,\n",
       " '他': 39,\n",
       " '意识形态': 40,\n",
       " '显示': 41,\n",
       " '对': 42,\n",
       " '债务': 43,\n",
       " '新': 44,\n",
       " '市场': 45,\n",
       " '战略': 46,\n",
       " '相反': 47,\n",
       " '把': 48,\n",
       " '然而': 49,\n",
       " '资本主义': 50,\n",
       " '其': 51,\n",
       " '明显': 52,\n",
       " '绝对': 53,\n",
       " '更': 54,\n",
       " '崩溃': 55,\n",
       " '改变': 56,\n",
       " '1931': 57,\n",
       " '集团': 58,\n",
       " '看': 59,\n",
       " '目前': 60,\n",
       " '了解': 61,\n",
       " '大多数': 62,\n",
       " '感受': 63,\n",
       " '似乎': 64,\n",
       " '民主': 65,\n",
       " '欧洲': 66,\n",
       " '一些': 67,\n",
       " '自由民主': 68,\n",
       " '后': 69,\n",
       " '信心': 70,\n",
       " '行动': 71,\n",
       " '周期性': 72,\n",
       " '寻找': 73,\n",
       " '沉重': 74,\n",
       " '如今': 75,\n",
       " '威胁': 76,\n",
       " '危机': 77,\n",
       " '或': 78,\n",
       " '想到': 79,\n",
       " '视为': 80,\n",
       " '来说': 81,\n",
       " '任何': 82,\n",
       " '宽心': 83,\n",
       " '完全': 84,\n",
       " '看似': 85,\n",
       " '经济危机': 86,\n",
       " '人们': 87,\n",
       " '让': 88,\n",
       " '重要': 89,\n",
       " '意义': 90,\n",
       " '1982': 91,\n",
       " '困境': 92,\n",
       " '支持者': 93,\n",
       " '将': 94,\n",
       " '后果': 95,\n",
       " '预期': 96,\n",
       " '更大': 97,\n",
       " '发人深省': 98,\n",
       " '已经': 99,\n",
       " '）': 100,\n",
       " '能': 101,\n",
       " '东西方': 102,\n",
       " '历史': 103,\n",
       " '从': 104,\n",
       " '充分': 105,\n",
       " '责任': 106,\n",
       " '发生': 107,\n",
       " '作为': 108,\n",
       " '比作': 109,\n",
       " '与此相反': 110,\n",
       " '国家': 111,\n",
       " '区分': 112,\n",
       " '但': 113,\n",
       " '为': 114,\n",
       " '学家': 115,\n",
       " '被': 116,\n",
       " '社会主义': 117,\n",
       " '却': 118,\n",
       " '情况': 119,\n",
       " '确实': 120,\n",
       " '扩展': 121,\n",
       " '努力': 122,\n",
       " '输家': 123,\n",
       " '政治': 124,\n",
       " '2009': 125,\n",
       " '民族主义': 126,\n",
       " '两极化': 127,\n",
       " '经济': 128,\n",
       " '很': 129,\n",
       " '首先': 130,\n",
       " '急需': 131,\n",
       " '最': 132,\n",
       " '推向': 133,\n",
       " '影响': 134,\n",
       " '1929': 135,\n",
       " '一': 136,\n",
       " '形式': 137,\n",
       " '典型': 138,\n",
       " '改革': 139,\n",
       " '变得': 140,\n",
       " '精心策划': 141,\n",
       " '其它': 142,\n",
       " '大': 143,\n",
       " '的': 144,\n",
       " '表面': 145,\n",
       " '希望': 146,\n",
       " '就是': 147,\n",
       " '尽管': 148,\n",
       " '几十年': 149,\n",
       " '要': 150,\n",
       " '比': 151,\n",
       " '许多': 152,\n",
       " '升级': 153,\n",
       " '随着': 154,\n",
       " '（': 155,\n",
       " '倒塌': 156,\n",
       " '1989': 157,\n",
       " '这样': 158,\n",
       " '不同': 159,\n",
       " '总统': 160,\n",
       " '开始': 161,\n",
       " '衰退': 162,\n",
       " '起来': 163,\n",
       " '人为': 164,\n",
       " '不': 165,\n",
       " '意味着': 166,\n",
       " '当时': 167,\n",
       " '走出': 168,\n",
       " '利用': 169,\n",
       " '随后': 170,\n",
       " '以及': 171,\n",
       " '倾向': 172,\n",
       " '克制': 173,\n",
       " '铺平道路': 174,\n",
       " '具体化': 175,\n",
       " '公司': 176,\n",
       " '势态': 177,\n",
       " '过度': 178,\n",
       " '无论是': 179,\n",
       " '2008': 180,\n",
       " '推崇': 181,\n",
       " '和平统一': 182,\n",
       " '地域': 183,\n",
       " '相比': 184,\n",
       " '自然': 185,\n",
       " '我们': 186,\n",
       " '另': 187,\n",
       " '会为': 188,\n",
       " '仍然': 189,\n",
       " '得到': 190,\n",
       " '金融': 191,\n",
       " '由': 192,\n",
       " '关系': 193,\n",
       " '有助于': 194,\n",
       " '公平': 195,\n",
       " '多': 196,\n",
       " '得': 197,\n",
       " '都': 198,\n",
       " '不断': 199,\n",
       " '着': 200,\n",
       " '这次': 201,\n",
       " '恐外': 202,\n",
       " '从而': 203,\n",
       " '一种': 204,\n",
       " '边缘': 205,\n",
       " '革命': 206,\n",
       " '没有': 207,\n",
       " '也许': 208,\n",
       " '以': 209,\n",
       " '，': 210,\n",
       " '并': 211,\n",
       " '转折点': 212,\n",
       " '时机': 213,\n",
       " '看见': 214,\n",
       " '人': 215,\n",
       " '心情': 216,\n",
       " '里根': 217,\n",
       " '；': 218,\n",
       " '方面': 219,\n",
       " '有些': 220,\n",
       " '划时代': 221,\n",
       " '负': 222,\n",
       " '坚不可摧': 223,\n",
       " '年': 224,\n",
       " '优越性': 225,\n",
       " '雷曼': 226,\n",
       " '所': 227,\n",
       " '理想': 228,\n",
       " '制度': 229,\n",
       " '时期': 230,\n",
       " '-': 231,\n",
       " '结束': 232,\n",
       " '年份': 233,\n",
       " '结构性': 234,\n",
       " '象征': 235,\n",
       " '蔓延': 236,\n",
       " '可能': 237,\n",
       " '社会': 238,\n",
       " '对于': 239,\n",
       " '全球': 240,\n",
       " '会': 241,\n",
       " '把视': 242,\n",
       " '这一': 243,\n",
       " '如果': 244,\n",
       " '却是': 245,\n",
       " '捍卫': 246,\n",
       " '还是': 247,\n",
       " '分裂': 248,\n",
       " '而': 249,\n",
       " '其二': 250,\n",
       " '加深': 251,\n",
       " '即使': 252,\n",
       " '名义': 253,\n",
       " '我': 254,\n",
       " '兄弟': 255,\n",
       " '成果': 256,\n",
       " '机构': 257,\n",
       " '受到': 258,\n",
       " '带来': 259,\n",
       " '要么': 260,\n",
       " '些': 261,\n",
       " '这': 262,\n",
       " '下正': 263,\n",
       " '来': 264,\n",
       " '安心': 265,\n",
       " '仍': 266,\n",
       " '战胜': 267,\n",
       " '不是': 268,\n",
       " '取代': 269,\n",
       " '两者': 270,\n",
       " '?': 271,\n",
       " '趋势': 272,\n",
       " '包括': 273,\n",
       " '自由': 274,\n",
       " '鸿沟': 275,\n",
       " '苏联': 276,\n",
       " '表现': 277,\n",
       " '倒闭': 278,\n",
       " '也': 279,\n",
       " '良好': 280,\n",
       " '因为': 281,\n",
       " '美国': 282,\n",
       " '令人': 283,\n",
       " '上': 284,\n",
       " '自由市场': 285,\n",
       " '的话': 286,\n",
       " '：': 287,\n",
       " '很多': 288,\n",
       " '谨慎': 289,\n",
       " '和': 290,\n",
       " '避免': 291,\n",
       " '<pad>': 0,\n",
       " '<sos>': 292}"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['巴黎', '-', '随着', '经济危机', '不断']"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_words_list[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> At the start of the crisis , many people likened it to 1982 or 1973 , which was reassuring , because both dates refer to classical cyclical downturns . <EOS>\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(sentence):\n",
    "    indices = sentence_to_indices(sentence, chinese_vocab)\n",
    "    tensor = to_tensor(indices)\n",
    "    translation = translate(encoder, decoder, tensor, chinese_index_vocab, english_index_vocab)\n",
    "    return translation\n",
    "\n",
    "# 翻译示例\n",
    "print(translate_sentence(zh_words_list[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch size 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 编码器\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)  # Set batch_first to True\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)  # input: (batch_size, seq_len)\n",
    "        output, hidden = self.gru(embedded, hidden)  # No need to permute\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "# 解码器\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)  # Set batch_first to True\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input)  # input: (batch_size,seq)\n",
    "#         output = output.unsqueeze(1)  # Change to (batch_size, 1, hidden_size)\n",
    "        output = nn.functional.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)  # No need to permute\n",
    "        output = self.softmax(self.out(output.squeeze(1)))  # Squeeze to remove the sequence length dimension\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=10):\n",
    "    batch_size = input_tensor.size(0)\n",
    "    input_length = input_tensor.size(1)\n",
    "    target_length = target_tensor.size(1)\n",
    "\n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # Encode the input sequence batch\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "\n",
    "    # Prepare the initial decoder input (start with SOS tokens for each sequence in the batch)\n",
    "    decoder_input = torch.tensor([[SOS_token] * batch_size], device=device).transpose(0, 1)  # Shape: (batch_size, 1)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Assuming teacher forcing is not used here. If desired, it can be included with a certain probability.\n",
    "    for di in range(target_length):\n",
    "#         print(decoder_input.shape)\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.detach()  # Prepare next input\n",
    "#         print(decoder_output.shape,target_tensor[:, di].shape,decoder_input.shape)\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[:, di])\n",
    "\n",
    "        # Optionally, stop when all sequences in the batch reach EOS token\n",
    "        if (decoder_input == EOS_token).all():\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / (target_length * batch_size)  # Normalize loss by total number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_batch(batch_size, max_length, vocabulary_size):\n",
    "    \"\"\" Generate a random batch of sequences for training. \"\"\"\n",
    "    # Random sequences of random lengths\n",
    "    sequences = torch.randint(2, vocabulary_size, (batch_size, max_length)).to(device)\n",
    "    return sequences\n",
    "\n",
    "# Example of generating a batch\n",
    "input_tensor = generate_fake_batch(batch_size, max_length, input_size)\n",
    "target_tensor = generate_fake_batch(batch_size, max_length, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 4, 9, 8, 2, 7],\n",
       "        [6, 6, 7, 6, 8, 2, 5],\n",
       "        [9, 5, 6, 7, 3, 4, 4],\n",
       "        [4, 5, 5, 5, 7, 7, 5],\n",
       "        [3, 5, 6, 2, 5, 2, 2]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4680\n",
      "Epoch 2, Loss: 0.5614\n",
      "Epoch 3, Loss: 0.4901\n",
      "Epoch 4, Loss: 0.5791\n",
      "Epoch 5, Loss: 0.5343\n",
      "Epoch 6, Loss: 0.4939\n",
      "Epoch 7, Loss: 0.4654\n",
      "Epoch 8, Loss: 0.4840\n",
      "Epoch 9, Loss: 0.4593\n",
      "Epoch 10, Loss: 0.4287\n",
      "Average Loss: 0.4964\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(encoder, decoder, input_tensor, target_tensor):\n",
    "    loss_total = 0  # Track loss\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Generate new data for each epoch\n",
    "        input_tensor = generate_fake_batch(batch_size, max_length, input_size)\n",
    "        target_tensor = generate_fake_batch(batch_size, max_length, output_size)\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\n",
    "        loss_total += loss\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss:.4f}')\n",
    "\n",
    "    print(f'Average Loss: {loss_total / epochs:.4f}')\n",
    "\n",
    "# Call the training function\n",
    "train_epoch(encoder, decoder, input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch_len 版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden, input_lengths):\n",
    "        embedded = self.embedding(input)  # input: (batch_size, seq_len)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, hidden = self.gru(packed, hidden)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input)  # input: (batch_size, 1)\n",
    "        output = nn.functional.relu(output)\n",
    "#         print('input shape, output shape 111',input.shape,output.shape)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "#         print('input shape, output shape  222',input.shape,output.shape)\n",
    "        output = self.softmax(self.out(output.squeeze(1)))  # Make sure output is squeezed\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, input_lengths, output_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=10):\n",
    "    batch_size = input_tensor.size(0)\n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # 编码器处理输入\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden, input_lengths)\n",
    "\n",
    "    # 解码器的初始输入\n",
    "    decoder_input = torch.tensor([[SOS_token] * batch_size], device=device).transpose(0, 1)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # 解码器生成输出\n",
    "    for di in range(max(output_lengths)):  # 使用最大输出长度\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        decoder_input = decoder_output.topk(1)[1].detach()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            if di < output_lengths[i]:  # 只计算有效长度内的损失\n",
    "                loss += criterion(decoder_output[i], target_tensor[i, di])\n",
    "\n",
    "        if (decoder_input == EOS_token).all():\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / sum(output_lengths)  # 按有效输出长度归一化损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'en' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-445-17f11b3ff200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# input_tensor, target_tensor, input_lengths, target_lengths = generate_batch(batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzhst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'en' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# 假设的词汇大小和隐藏层大小\n",
    "\n",
    "zhst = SentencesTool(zh_sentences)\n",
    "enst = SentencesTool(en_sentences)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = len(zhst.vocab)  # Example vocabulary size for encoder\n",
    "output_size = len(enst.vocab)  # Example vocabulary size for decoder\n",
    "\n",
    "# input_size = 10\n",
    "# output_size = 10\n",
    "hidden_size = 64\n",
    "\n",
    "# 特殊符号定义\n",
    "SOS_token = 0  # 句子开始符号\n",
    "EOS_token = 1  # 句子结束符号\n",
    "\n",
    "# 实例化模型\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(hidden_size, output_size).to(device)\n",
    "\n",
    "# 优化器\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.01)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.01)\n",
    "\n",
    "# 损失函数\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# 创建一些虚拟数据\n",
    "def generate_batch(batch_size):\n",
    "    max_length = 10\n",
    "    input_tensor = torch.LongTensor(batch_size, max_length).random_(2, input_size).to(device)\n",
    "    target_tensor = torch.LongTensor(batch_size, max_length).random_(2, output_size).to(device)\n",
    "    input_lengths = torch.LongTensor([random.randint(5, max_length) for _ in range(batch_size)]).to(device)\n",
    "    target_lengths = torch.LongTensor([random.randint(5, max_length) for _ in range(batch_size)]).to(device)\n",
    "    return input_tensor, target_tensor, input_lengths, target_lengths\n",
    "\n",
    "batch_size = 5\n",
    "# input_tensor, target_tensor, input_lengths, target_lengths = generate_batch(batch_size)\n",
    "input_tensor,input_lengths = zhst.tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tensor, target_lengths = enst.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor,input_lengths = input_tensor[:batch_size],input_lengths[:batch_size]\n",
    "target_tensor, target_lengths = target_tensor[:batch_size], target_lengths[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[24067,  2141, 88266,  ...,     0,     0,     0],\n",
       "         [62243, 53690, 66285,  ...,     0,     0,     0],\n",
       "         [73629,  5698, 92104,  ...,     0,     0,     0],\n",
       "         [69867, 59048, 70599,  ...,     0,     0,     0],\n",
       "         [79662, 70599, 60418,  ...,     0,     0,     0]]),\n",
       " tensor([ 7, 30, 38, 45, 24]))"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor,input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.073349952697754\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "loss = train(input_tensor, target_tensor, input_lengths, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "\n",
    "# 损失函数\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.363315105438232\n",
      "Loss: 6.2935471534729\n",
      "Loss: 6.323805809020996\n",
      "Loss: 6.305229663848877\n",
      "Loss: 6.357680320739746\n",
      "Loss: 6.141800880432129\n",
      "Loss: 6.139320373535156\n",
      "Loss: 6.227478504180908\n",
      "Loss: 6.275618553161621\n",
      "Loss: 6.172341346740723\n",
      "Loss: 6.347276210784912\n",
      "Loss: 6.4681220054626465\n",
      "Loss: 6.454342365264893\n",
      "Loss: 6.718870639801025\n",
      "Loss: 6.857548713684082\n",
      "Loss: 6.773695468902588\n",
      "Loss: 6.49314546585083\n",
      "Loss: 6.724074363708496\n",
      "Loss: 6.837404727935791\n",
      "Loss: 6.943368434906006\n",
      "Loss: 6.547971248626709\n",
      "Loss: 6.730683326721191\n",
      "Loss: 6.6053786277771\n",
      "Loss: 6.675388336181641\n",
      "Loss: 6.721620082855225\n",
      "Loss: 7.104621887207031\n",
      "Loss: 7.006012439727783\n",
      "Loss: 6.811808109283447\n",
      "Loss: 6.747270107269287\n",
      "Loss: 7.0230278968811035\n",
      "Loss: 6.7035417556762695\n",
      "Loss: 6.956991672515869\n",
      "Loss: 6.6015777587890625\n",
      "Loss: 6.8602190017700195\n",
      "Loss: 6.762325763702393\n",
      "Loss: 6.7107930183410645\n",
      "Loss: 6.629238605499268\n",
      "Loss: 6.766843318939209\n",
      "Loss: 6.88611364364624\n",
      "Loss: 7.025515556335449\n",
      "Loss: 7.257446765899658\n",
      "Loss: 7.102090358734131\n",
      "Loss: 6.991309642791748\n",
      "Loss: 6.880009174346924\n",
      "Loss: 6.821377754211426\n",
      "Loss: 6.83026123046875\n",
      "Loss: 6.692685604095459\n",
      "Loss: 6.67300271987915\n",
      "Loss: 6.683180809020996\n",
      "Loss: 6.776037216186523\n",
      "Loss: 6.887969970703125\n",
      "Loss: 6.915509223937988\n",
      "Loss: 6.867613315582275\n",
      "Loss: 6.782782077789307\n",
      "Loss: 6.571816444396973\n",
      "Loss: 7.039750576019287\n",
      "Loss: 6.5128631591796875\n",
      "Loss: 6.540425777435303\n",
      "Loss: 6.7718377113342285\n",
      "Loss: 6.680942058563232\n",
      "Loss: 6.515820026397705\n",
      "Loss: 6.968789577484131\n",
      "Loss: 6.459348678588867\n",
      "Loss: 6.429876327514648\n",
      "Loss: 6.6575140953063965\n",
      "Loss: 6.516212463378906\n",
      "Loss: 6.569733142852783\n",
      "Loss: 6.776035308837891\n",
      "Loss: 6.6627326011657715\n",
      "Loss: 6.714951515197754\n",
      "Loss: 6.722715854644775\n",
      "Loss: 6.919351577758789\n",
      "Loss: 6.970373630523682\n",
      "Loss: 6.6083526611328125\n",
      "Loss: 6.703805446624756\n",
      "Loss: 6.7451171875\n",
      "Loss: 6.810898780822754\n",
      "Loss: 6.624625205993652\n",
      "Loss: 6.707365036010742\n",
      "Loss: 6.839631080627441\n",
      "Loss: 7.138201713562012\n",
      "Loss: 6.624554634094238\n",
      "Loss: 6.620912551879883\n",
      "Loss: 6.684226989746094\n",
      "Loss: 6.69551420211792\n",
      "Loss: 6.8553385734558105\n",
      "Loss: 6.83405065536499\n",
      "Loss: 6.481200695037842\n",
      "Loss: 6.487667560577393\n",
      "Loss: 6.757015705108643\n",
      "Loss: 7.0403361320495605\n",
      "Loss: 6.826805114746094\n",
      "Loss: 6.440674781799316\n",
      "Loss: 6.5262064933776855\n",
      "Loss: 6.798347473144531\n",
      "Loss: 6.67692232131958\n",
      "Loss: 6.638891220092773\n",
      "Loss: 6.540221691131592\n",
      "Loss: 6.869771957397461\n",
      "Loss: 6.683051109313965\n",
      "Loss: 6.774408340454102\n",
      "Loss: 6.526919841766357\n",
      "Loss: 6.6145429611206055\n",
      "Loss: 6.766429901123047\n",
      "Loss: 6.394925594329834\n",
      "Loss: 6.538279056549072\n",
      "Loss: 6.655675888061523\n",
      "Loss: 6.859565258026123\n",
      "Loss: 6.6111931800842285\n",
      "Loss: 6.781493186950684\n",
      "Loss: 6.751119136810303\n",
      "Loss: 6.651870250701904\n",
      "Loss: 6.791252136230469\n",
      "Loss: 6.532045841217041\n",
      "Loss: 6.679943084716797\n",
      "Loss: 6.7065606117248535\n",
      "Loss: 6.55168342590332\n",
      "Loss: 6.676589012145996\n",
      "Loss: 7.0987138748168945\n",
      "Loss: 6.673173904418945\n",
      "Loss: 6.858807563781738\n",
      "Loss: 7.058838844299316\n",
      "Loss: 6.903945446014404\n",
      "Loss: 6.925490379333496\n",
      "Loss: 6.6816840171813965\n",
      "Loss: 6.633397579193115\n",
      "Loss: 6.8234758377075195\n",
      "Loss: 6.751100540161133\n",
      "Loss: 6.782322883605957\n",
      "Loss: 6.959286689758301\n",
      "Loss: 6.622400760650635\n",
      "Loss: 6.990128993988037\n",
      "Loss: 6.579662799835205\n",
      "Loss: 6.9350175857543945\n",
      "Loss: 6.8738555908203125\n",
      "Loss: 6.728034496307373\n",
      "Loss: 6.731057643890381\n",
      "Loss: 6.58151388168335\n",
      "Loss: 6.715462684631348\n",
      "Loss: 6.781252384185791\n",
      "Loss: 6.577158451080322\n",
      "Loss: 6.638530254364014\n",
      "Loss: 6.708644866943359\n",
      "Loss: 6.995742321014404\n",
      "Loss: 6.621944427490234\n",
      "Loss: 6.900588035583496\n",
      "Loss: 7.036019802093506\n",
      "Loss: 6.839605331420898\n",
      "Loss: 6.802015781402588\n",
      "Loss: 6.865862846374512\n",
      "Loss: 6.999306678771973\n",
      "Loss: 6.708532810211182\n",
      "Loss: 6.762307167053223\n",
      "Loss: 6.7845892906188965\n",
      "Loss: 6.784864902496338\n",
      "Loss: 6.894998550415039\n",
      "Loss: 6.86631965637207\n",
      "Loss: 7.136867523193359\n",
      "Loss: 6.829811096191406\n",
      "Loss: 6.855641841888428\n",
      "Loss: 6.963149547576904\n",
      "Loss: 7.176844120025635\n",
      "Loss: 6.962678909301758\n",
      "Loss: 7.0877509117126465\n",
      "Loss: 7.245007514953613\n",
      "Loss: 7.0107293128967285\n",
      "Loss: 7.043623447418213\n",
      "Loss: 6.692122936248779\n",
      "Loss: 6.992412090301514\n",
      "Loss: 6.953486919403076\n",
      "Loss: 6.921473503112793\n",
      "Loss: 6.733234405517578\n",
      "Loss: 6.917731285095215\n",
      "Loss: 6.628203392028809\n",
      "Loss: 6.765244007110596\n",
      "Loss: 6.738156795501709\n",
      "Loss: 6.91599702835083\n",
      "Loss: 6.642406463623047\n",
      "Loss: 6.735754489898682\n",
      "Loss: 6.810708522796631\n",
      "Loss: 6.822346210479736\n",
      "Loss: 6.729852199554443\n",
      "Loss: 7.212001323699951\n",
      "Loss: 6.979021072387695\n",
      "Loss: 6.813241958618164\n",
      "Loss: 6.6100358963012695\n",
      "Loss: 7.072962760925293\n",
      "Loss: 7.007970333099365\n",
      "Loss: 6.869743347167969\n",
      "Loss: 6.873200416564941\n",
      "Loss: 6.966241836547852\n",
      "Loss: 6.743324279785156\n",
      "Loss: 6.832393646240234\n",
      "Loss: 6.884451389312744\n",
      "Loss: 6.72450065612793\n",
      "Loss: 7.005912780761719\n",
      "Loss: 6.81853723526001\n",
      "Loss: 6.9477643966674805\n",
      "Loss: 7.076380729675293\n",
      "Loss: 7.06412935256958\n",
      "Loss: 6.864109039306641\n",
      "Loss: 6.8794989585876465\n",
      "Loss: 7.062367916107178\n",
      "Loss: 6.725939750671387\n",
      "Loss: 6.85598611831665\n",
      "Loss: 7.255154609680176\n",
      "Loss: 7.085005283355713\n",
      "Loss: 7.045530319213867\n",
      "Loss: 7.228945732116699\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-457-5ee7323533da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzhst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzhst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loss: {loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-451-47d81eb13da4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, input_lengths, output_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(100):\n",
    "    b = 64\n",
    "    for i in range(len(zhst.tensors[0])//b):\n",
    "        input_tensor,input_lengths = zhst.tensors[0][i*b:(i+1)*b],zhst.tensors[1][i*b:(i+1)*b]\n",
    "        target_tensor, target_lengths = enst.tensors[0][i*b:(i+1)*b],enst.tensors[1][i*b:(i+1)*b]\n",
    "        loss = train(input_tensor, target_tensor, input_lengths, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_path = '/Users/liuchu/linear_algebra_strang/translate/中英翻译数据集/train/news-commentary-v13.zh-en.en'\n",
    "zh_path = '/Users/liuchu/linear_algebra_strang/translate/中英翻译数据集/train/news-commentary-v13.zh-en.zh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_en_sentences():\n",
    "    arr = []\n",
    "    with open(en_path,'r') as f:\n",
    "        count = 0\n",
    "        for line in f.readlines():\n",
    "            arr.append(line.strip())\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zh_sentences():\n",
    "    arr = []\n",
    "    with open(zh_path,'r') as f:\n",
    "        count = 0\n",
    "        for line in f.readlines():\n",
    "            arr.append(line.strip())\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentences = get_en_sentences()\n",
    "zh_sentences = get_zh_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 制作批处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1929 or 1989?',\n",
       " 'PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.',\n",
       " 'At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.',\n",
       " 'Today, the mood is much grimmer, with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.',\n",
       " 'The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).',\n",
       " 'Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.',\n",
       " 'For geo-strategists, however, the year that naturally comes to mind, in both politics and economics, is 1989.',\n",
       " 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       " 'Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and reassuring institution of financial capitalism.',\n",
       " 'Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose unfolding consequences will be felt for decades.']"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1929 年 还是 1989 年 ?',\n",
       " '巴黎 - 随着 经济危机 不断 加深 和 蔓延 ， 整个 世界 一直 在 寻找 历史 上 的 类似 事件 希望 有助于 我们 了解 目前 正在 发生 的 情况 。',\n",
       " '一 开始 ， 很多 人 把 这次 危机 比作 1982 年 或 1973 年 所 发生 的 情况 ， 这样 得 类比 是 令人 宽心 的 ， 因为 这 两段 时期 意味着 典型 的 周期性 衰退 。',\n",
       " '如今 人们 的 心情 却是 沉重 多 了 ， 许多 人 开始 把 这次 危机 与 1929 年 和 1931 年 相比 ， 即使 一些 国家 政府 的 表现 仍然 似乎 把视 目前 的 情况 为 是 典型 的 而 看见 的 衰退 。',\n",
       " '目前 的 趋势 是 ， 要么 是 过度 的 克制 （ 欧洲 ）， 要么 是 努力 的 扩展 （ 美国 ）。',\n",
       " '欧洲 在 避免 债务 和 捍卫 欧元 的 名义 下正 变得 谨慎 ， 而 美国 已经 在 许多 方面 行动 起来 ， 以 利用 这一 理想 的 时机 来 实行 急需 的 结构性 改革 。',\n",
       " '然而 ， 作为 地域 战略 学家 ， 无论是 从 政治 意义 还是 从 经济 意义 上 ， 让 我 自然 想到 的 年份 是 1989 年 。',\n",
       " '当然 ， 雷曼 兄弟 公司 的 倒闭 和 柏林墙 的 倒塌 没有 任何 关系 。',\n",
       " '事实上 ， 从 表面 上 看 ， 两者 似乎 是 完全 是 相反 的 ： 一个 是 象征 着 压抑 和 人为 分裂 的 柏林墙 的 倒塌 ， 而 另 一个 是 看似 坚不可摧 的 并 令人 安心 的 金融 资本主义 机构 的 倒塌 。',\n",
       " '然而 ， 和 1989 年 一样 ， 2008 - 2009 年 很 可能 也 能 被 视为 一个 划时代 的 改变 ， 其 带来 的 发人深省 的 后果 将 在 几十年 后 仍 能 让 我们 感受 得到 。']"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_sentence_with_punctuation(sentence):\n",
    "    # 使用正则表达式将句子分割成单词和标点符号\n",
    "    words = re.findall(r'\\w+|[^\\w\\s]', sentence)\n",
    "    return words\n",
    "\n",
    "# # 测试\n",
    "# sentence = \"PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.\"\n",
    "# word_list = split_sentence_with_punctuation(sentence)\n",
    "# print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_words_list = [split_sentence_with_punctuation(s) for s in zh_sentences]\n",
    "en_words_list = [split_sentence_with_punctuation(s) for s in en_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_max_len = max(len(alist) for alist in zh_words_list)\n",
    "en_mac_len = max(len(alist) for alist in en_words_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencesTool:\n",
    "    \n",
    "    def __init__(self,sentences):\n",
    "        self.words_list = [split_sentence_with_punctuation(s) for s in sentences]\n",
    "        self.max_len = max(len(alist) for alist in self.words_list) + 1\n",
    "        self.vocab = ['<pad>','<sos>','<eos>'] + list(set(word for words in self.words_list for word in words))\n",
    "        self.index2word = {i:word for i,word in enumerate(self.vocab)}\n",
    "        self.word2index = {word:i for i,word in self.index2word.items()} \n",
    "        self.tensors = self.totensor()\n",
    "    \n",
    "    def totensor(self):\n",
    "        ans = []\n",
    "        batch_len = []\n",
    "        for words in self.words_list:\n",
    "            indexs = [self.word2index[word] for word in words +['<eos>']+ ['<pad>']*(self.max_len-len(words)-1)]\n",
    "            ans.append(indexs)\n",
    "            batch_len.append(len(words)+1)\n",
    "        return torch.tensor(ans, dtype=torch.long, device=device),torch.tensor(batch_len, dtype=torch.long, device=device)\n",
    "    \n",
    "    def toindex(self,word):\n",
    "        return self.word2index[word]\n",
    "    \n",
    "    def toword(self,index):\n",
    "        return self.index2word[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = SentencesTool(zh_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[24066,  2140, 88265,  ...,     0,     0,     0],\n",
       "         [62242, 53689, 66284,  ...,     0,     0,     0],\n",
       "         [73628,  5697, 92103,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [ 9636, 65812, 75159,  ...,     0,     0,     0],\n",
       "         [ 1322, 81455, 86240,  ...,     0,     0,     0],\n",
       "         [39343, 92103, 17799,  ...,     0,     0,     0]]),\n",
       " tensor([ 7, 30, 38,  ..., 29, 24, 42]))"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sentences = en_sentences\n",
    "target_sentences = zh_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 编码器\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "# 解码器\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = nn.functional.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=10):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS_token是起始符号的标记\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:  # EOS_token是结束符号的标记\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorFromSentence(vocab, sentence):\n",
    "    indices = [vocab[char] for char in sentence if char in vocab]  # Ensure char is in vocab\n",
    "    indices.append(vocab['<eos>'])  # Append EOS token\n",
    "    return torch.tensor(indices, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def translate_sentence(sentence, encoder, decoder, input_vocab, output_vocab):\n",
    "    input_tensor = tensorFromSentence(input_vocab, sentence)\n",
    "    translated_sentence = translate(encoder, decoder, input_tensor, input_vocab, output_vocab)\n",
    "    return translated_sentence\n",
    "\n",
    "def translate(encoder, decoder, input_tensor, input_vocab, output_vocab, max_length=100):\n",
    "    with torch.no_grad():\n",
    "        input_length = input_tensor.size(0)\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        # Forward pass through encoder\n",
    "        for ei in range(input_length):\n",
    "            _, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "        # Decoder's first input is SOS token\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_word = output_vocab[topi.item()]\n",
    "                decoded_words.append(decoded_word)\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return ' '.join(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设语料数据如下，这里我们使用了非常简单的例子\n",
    "chinese_sentences = zh_words_list[:]\n",
    "english_sentences = en_words_list[:]\n",
    "\n",
    "# 为了简化问题，我们不使用词嵌入，而是直接根据字母或汉字进行索引\n",
    "def create_dict(sentences):\n",
    "    vocab = set(w for s in sentences for w in s)\n",
    "    char_to_index = {char: i+1 for i, char in enumerate(vocab)}  # +1因为我们留出0作为padding\n",
    "    char_to_index['<pad>'] = 0\n",
    "    index_to_char = {i: char for char, i in char_to_index.items()}\n",
    "    return char_to_index, index_to_char\n",
    "\n",
    "chinese_vocab, chinese_index_vocab = create_dict(chinese_sentences)\n",
    "chinese_vocab['<sos>'] = max(chinese_vocab.values()) + 1\n",
    "chinese_index_vocab[chinese_vocab['<sos>']] = '<sos>'\n",
    "\n",
    "english_vocab, english_index_vocab = create_dict(english_sentences)\n",
    "\n",
    "# 将句子转换为索引\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab[char] for char in sentence]\n",
    "\n",
    "chinese_data = [sentence_to_indices(sentence, chinese_vocab) for sentence in chinese_sentences]\n",
    "english_data = [sentence_to_indices(sentence, english_vocab) for sentence in english_sentences]\n",
    "\n",
    "# 增加开始和结束标记\n",
    "\n",
    "SOS_token = english_vocab['<sos>'] = max(english_vocab.values()) + 1\n",
    "EOS_token = english_vocab['<eos>'] = max(english_vocab.values()) + 1\n",
    "english_index_vocab[SOS_token] = '<sos>'\n",
    "english_index_vocab[EOS_token] = '<eos>'\n",
    "english_data = [[SOS_token] + sentence + [EOS_token] for sentence in english_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63875, 63875)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_index_vocab),len(english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92865"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chinese_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据转换为torch张量\n",
    "def to_tensor(data):\n",
    "    return torch.tensor(data, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "chinese_tensors = [to_tensor(data) for data in chinese_data]\n",
    "english_tensors = [to_tensor(data) for data in english_data]\n",
    "\n",
    "# 训练参数设置\n",
    "num_epochs = 100\n",
    "\n",
    "encoder = Encoder(len(chinese_vocab), 256).to(device)\n",
    "decoder = Decoder(256, len(english_vocab)).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-422-ca9cbf82f3e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mch_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchinese_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-392-ed0640575a3f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for ch_tensor, en_tensor in zip(chinese_tensors, english_tensors):\n",
    "        loss = train(ch_tensor, en_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        total_loss += loss\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(chinese_sentences):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1929', '年', '还是', '1989', '年', '?'],\n",
       " ['巴黎',\n",
       "  '-',\n",
       "  '随着',\n",
       "  '经济危机',\n",
       "  '不断',\n",
       "  '加深',\n",
       "  '和',\n",
       "  '蔓延',\n",
       "  '，',\n",
       "  '整个',\n",
       "  '世界',\n",
       "  '一直',\n",
       "  '在',\n",
       "  '寻找',\n",
       "  '历史',\n",
       "  '上',\n",
       "  '的',\n",
       "  '类似',\n",
       "  '事件',\n",
       "  '希望',\n",
       "  '有助于',\n",
       "  '我们',\n",
       "  '了解',\n",
       "  '目前',\n",
       "  '正在',\n",
       "  '发生',\n",
       "  '的',\n",
       "  '情况',\n",
       "  '。'],\n",
       " ['一',\n",
       "  '开始',\n",
       "  '，',\n",
       "  '很多',\n",
       "  '人',\n",
       "  '把',\n",
       "  '这次',\n",
       "  '危机',\n",
       "  '比作',\n",
       "  '1982',\n",
       "  '年',\n",
       "  '或',\n",
       "  '1973',\n",
       "  '年',\n",
       "  '所',\n",
       "  '发生',\n",
       "  '的',\n",
       "  '情况',\n",
       "  '，',\n",
       "  '这样',\n",
       "  '得',\n",
       "  '类比',\n",
       "  '是',\n",
       "  '令人',\n",
       "  '宽心',\n",
       "  '的',\n",
       "  '，',\n",
       "  '因为',\n",
       "  '这',\n",
       "  '两段',\n",
       "  '时期',\n",
       "  '意味着',\n",
       "  '典型',\n",
       "  '的',\n",
       "  '周期性',\n",
       "  '衰退',\n",
       "  '。'],\n",
       " ['如今',\n",
       "  '人们',\n",
       "  '的',\n",
       "  '心情',\n",
       "  '却是',\n",
       "  '沉重',\n",
       "  '多',\n",
       "  '了',\n",
       "  '，',\n",
       "  '许多',\n",
       "  '人',\n",
       "  '开始',\n",
       "  '把',\n",
       "  '这次',\n",
       "  '危机',\n",
       "  '与',\n",
       "  '1929',\n",
       "  '年',\n",
       "  '和',\n",
       "  '1931',\n",
       "  '年',\n",
       "  '相比',\n",
       "  '，',\n",
       "  '即使',\n",
       "  '一些',\n",
       "  '国家',\n",
       "  '政府',\n",
       "  '的',\n",
       "  '表现',\n",
       "  '仍然',\n",
       "  '似乎',\n",
       "  '把视',\n",
       "  '目前',\n",
       "  '的',\n",
       "  '情况',\n",
       "  '为',\n",
       "  '是',\n",
       "  '典型',\n",
       "  '的',\n",
       "  '而',\n",
       "  '看见',\n",
       "  '的',\n",
       "  '衰退',\n",
       "  '。'],\n",
       " ['目前',\n",
       "  '的',\n",
       "  '趋势',\n",
       "  '是',\n",
       "  '，',\n",
       "  '要么',\n",
       "  '是',\n",
       "  '过度',\n",
       "  '的',\n",
       "  '克制',\n",
       "  '（',\n",
       "  '欧洲',\n",
       "  '）',\n",
       "  '，',\n",
       "  '要么',\n",
       "  '是',\n",
       "  '努力',\n",
       "  '的',\n",
       "  '扩展',\n",
       "  '（',\n",
       "  '美国',\n",
       "  '）',\n",
       "  '。'],\n",
       " ['欧洲',\n",
       "  '在',\n",
       "  '避免',\n",
       "  '债务',\n",
       "  '和',\n",
       "  '捍卫',\n",
       "  '欧元',\n",
       "  '的',\n",
       "  '名义',\n",
       "  '下正',\n",
       "  '变得',\n",
       "  '谨慎',\n",
       "  '，',\n",
       "  '而',\n",
       "  '美国',\n",
       "  '已经',\n",
       "  '在',\n",
       "  '许多',\n",
       "  '方面',\n",
       "  '行动',\n",
       "  '起来',\n",
       "  '，',\n",
       "  '以',\n",
       "  '利用',\n",
       "  '这一',\n",
       "  '理想',\n",
       "  '的',\n",
       "  '时机',\n",
       "  '来',\n",
       "  '实行',\n",
       "  '急需',\n",
       "  '的',\n",
       "  '结构性',\n",
       "  '改革',\n",
       "  '。'],\n",
       " ['然而',\n",
       "  '，',\n",
       "  '作为',\n",
       "  '地域',\n",
       "  '战略',\n",
       "  '学家',\n",
       "  '，',\n",
       "  '无论是',\n",
       "  '从',\n",
       "  '政治',\n",
       "  '意义',\n",
       "  '还是',\n",
       "  '从',\n",
       "  '经济',\n",
       "  '意义',\n",
       "  '上',\n",
       "  '，',\n",
       "  '让',\n",
       "  '我',\n",
       "  '自然',\n",
       "  '想到',\n",
       "  '的',\n",
       "  '年份',\n",
       "  '是',\n",
       "  '1989',\n",
       "  '年',\n",
       "  '。'],\n",
       " ['当然',\n",
       "  '，',\n",
       "  '雷曼',\n",
       "  '兄弟',\n",
       "  '公司',\n",
       "  '的',\n",
       "  '倒闭',\n",
       "  '和',\n",
       "  '柏林墙',\n",
       "  '的',\n",
       "  '倒塌',\n",
       "  '没有',\n",
       "  '任何',\n",
       "  '关系',\n",
       "  '。'],\n",
       " ['事实上',\n",
       "  '，',\n",
       "  '从',\n",
       "  '表面',\n",
       "  '上',\n",
       "  '看',\n",
       "  '，',\n",
       "  '两者',\n",
       "  '似乎',\n",
       "  '是',\n",
       "  '完全',\n",
       "  '是',\n",
       "  '相反',\n",
       "  '的',\n",
       "  '：',\n",
       "  '一个',\n",
       "  '是',\n",
       "  '象征',\n",
       "  '着',\n",
       "  '压抑',\n",
       "  '和',\n",
       "  '人为',\n",
       "  '分裂',\n",
       "  '的',\n",
       "  '柏林墙',\n",
       "  '的',\n",
       "  '倒塌',\n",
       "  '，',\n",
       "  '而',\n",
       "  '另',\n",
       "  '一个',\n",
       "  '是',\n",
       "  '看似',\n",
       "  '坚不可摧',\n",
       "  '的',\n",
       "  '并',\n",
       "  '令人',\n",
       "  '安心',\n",
       "  '的',\n",
       "  '金融',\n",
       "  '资本主义',\n",
       "  '机构',\n",
       "  '的',\n",
       "  '倒塌',\n",
       "  '。'],\n",
       " ['然而',\n",
       "  '，',\n",
       "  '和',\n",
       "  '1989',\n",
       "  '年',\n",
       "  '一样',\n",
       "  '，',\n",
       "  '2008',\n",
       "  '-',\n",
       "  '2009',\n",
       "  '年',\n",
       "  '很',\n",
       "  '可能',\n",
       "  '也',\n",
       "  '能',\n",
       "  '被',\n",
       "  '视为',\n",
       "  '一个',\n",
       "  '划时代',\n",
       "  '的',\n",
       "  '改变',\n",
       "  '，',\n",
       "  '其',\n",
       "  '带来',\n",
       "  '的',\n",
       "  '发人深省',\n",
       "  '的',\n",
       "  '后果',\n",
       "  '将',\n",
       "  '在',\n",
       "  '几十年',\n",
       "  '后',\n",
       "  '仍',\n",
       "  '能',\n",
       "  '让',\n",
       "  '我们',\n",
       "  '感受',\n",
       "  '得到',\n",
       "  '。']]"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_words_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1929 or 1989?',\n",
       " 'PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.',\n",
       " 'At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.',\n",
       " 'Today, the mood is much grimmer, with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.',\n",
       " 'The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).',\n",
       " 'Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.',\n",
       " 'For geo-strategists, however, the year that naturally comes to mind, in both politics and economics, is 1989.',\n",
       " 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       " 'Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and reassuring institution of financial capitalism.',\n",
       " 'Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose unfolding consequences will be felt for decades.']"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'难': 1,\n",
       " '。': 2,\n",
       " '巴黎': 3,\n",
       " '看来': 4,\n",
       " '现在': 5,\n",
       " '脆弱': 6,\n",
       " '一个': 7,\n",
       " '压抑': 8,\n",
       " '一样': 9,\n",
       " '在': 10,\n",
       " '两段': 11,\n",
       " '整个': 12,\n",
       " '事件': 13,\n",
       " '正在': 14,\n",
       " '1973': 15,\n",
       " '类比': 16,\n",
       " '当然': 17,\n",
       " '事情': 18,\n",
       " '军备竞赛': 19,\n",
       " '政府': 20,\n",
       " '与': 21,\n",
       " '欧元': 22,\n",
       " '事实上': 23,\n",
       " '赢家': 24,\n",
       " '实行': 25,\n",
       " '一直': 26,\n",
       " '柏林墙': 27,\n",
       " '只是': 28,\n",
       " '类似': 29,\n",
       " '是': 30,\n",
       " '当今': 31,\n",
       " '世界': 32,\n",
       " '或许': 33,\n",
       " '每个': 34,\n",
       " '了': 35,\n",
       " '解体': 36,\n",
       " '积极': 37,\n",
       " '中国': 38,\n",
       " '他': 39,\n",
       " '意识形态': 40,\n",
       " '显示': 41,\n",
       " '对': 42,\n",
       " '债务': 43,\n",
       " '新': 44,\n",
       " '市场': 45,\n",
       " '战略': 46,\n",
       " '相反': 47,\n",
       " '把': 48,\n",
       " '然而': 49,\n",
       " '资本主义': 50,\n",
       " '其': 51,\n",
       " '明显': 52,\n",
       " '绝对': 53,\n",
       " '更': 54,\n",
       " '崩溃': 55,\n",
       " '改变': 56,\n",
       " '1931': 57,\n",
       " '集团': 58,\n",
       " '看': 59,\n",
       " '目前': 60,\n",
       " '了解': 61,\n",
       " '大多数': 62,\n",
       " '感受': 63,\n",
       " '似乎': 64,\n",
       " '民主': 65,\n",
       " '欧洲': 66,\n",
       " '一些': 67,\n",
       " '自由民主': 68,\n",
       " '后': 69,\n",
       " '信心': 70,\n",
       " '行动': 71,\n",
       " '周期性': 72,\n",
       " '寻找': 73,\n",
       " '沉重': 74,\n",
       " '如今': 75,\n",
       " '威胁': 76,\n",
       " '危机': 77,\n",
       " '或': 78,\n",
       " '想到': 79,\n",
       " '视为': 80,\n",
       " '来说': 81,\n",
       " '任何': 82,\n",
       " '宽心': 83,\n",
       " '完全': 84,\n",
       " '看似': 85,\n",
       " '经济危机': 86,\n",
       " '人们': 87,\n",
       " '让': 88,\n",
       " '重要': 89,\n",
       " '意义': 90,\n",
       " '1982': 91,\n",
       " '困境': 92,\n",
       " '支持者': 93,\n",
       " '将': 94,\n",
       " '后果': 95,\n",
       " '预期': 96,\n",
       " '更大': 97,\n",
       " '发人深省': 98,\n",
       " '已经': 99,\n",
       " '）': 100,\n",
       " '能': 101,\n",
       " '东西方': 102,\n",
       " '历史': 103,\n",
       " '从': 104,\n",
       " '充分': 105,\n",
       " '责任': 106,\n",
       " '发生': 107,\n",
       " '作为': 108,\n",
       " '比作': 109,\n",
       " '与此相反': 110,\n",
       " '国家': 111,\n",
       " '区分': 112,\n",
       " '但': 113,\n",
       " '为': 114,\n",
       " '学家': 115,\n",
       " '被': 116,\n",
       " '社会主义': 117,\n",
       " '却': 118,\n",
       " '情况': 119,\n",
       " '确实': 120,\n",
       " '扩展': 121,\n",
       " '努力': 122,\n",
       " '输家': 123,\n",
       " '政治': 124,\n",
       " '2009': 125,\n",
       " '民族主义': 126,\n",
       " '两极化': 127,\n",
       " '经济': 128,\n",
       " '很': 129,\n",
       " '首先': 130,\n",
       " '急需': 131,\n",
       " '最': 132,\n",
       " '推向': 133,\n",
       " '影响': 134,\n",
       " '1929': 135,\n",
       " '一': 136,\n",
       " '形式': 137,\n",
       " '典型': 138,\n",
       " '改革': 139,\n",
       " '变得': 140,\n",
       " '精心策划': 141,\n",
       " '其它': 142,\n",
       " '大': 143,\n",
       " '的': 144,\n",
       " '表面': 145,\n",
       " '希望': 146,\n",
       " '就是': 147,\n",
       " '尽管': 148,\n",
       " '几十年': 149,\n",
       " '要': 150,\n",
       " '比': 151,\n",
       " '许多': 152,\n",
       " '升级': 153,\n",
       " '随着': 154,\n",
       " '（': 155,\n",
       " '倒塌': 156,\n",
       " '1989': 157,\n",
       " '这样': 158,\n",
       " '不同': 159,\n",
       " '总统': 160,\n",
       " '开始': 161,\n",
       " '衰退': 162,\n",
       " '起来': 163,\n",
       " '人为': 164,\n",
       " '不': 165,\n",
       " '意味着': 166,\n",
       " '当时': 167,\n",
       " '走出': 168,\n",
       " '利用': 169,\n",
       " '随后': 170,\n",
       " '以及': 171,\n",
       " '倾向': 172,\n",
       " '克制': 173,\n",
       " '铺平道路': 174,\n",
       " '具体化': 175,\n",
       " '公司': 176,\n",
       " '势态': 177,\n",
       " '过度': 178,\n",
       " '无论是': 179,\n",
       " '2008': 180,\n",
       " '推崇': 181,\n",
       " '和平统一': 182,\n",
       " '地域': 183,\n",
       " '相比': 184,\n",
       " '自然': 185,\n",
       " '我们': 186,\n",
       " '另': 187,\n",
       " '会为': 188,\n",
       " '仍然': 189,\n",
       " '得到': 190,\n",
       " '金融': 191,\n",
       " '由': 192,\n",
       " '关系': 193,\n",
       " '有助于': 194,\n",
       " '公平': 195,\n",
       " '多': 196,\n",
       " '得': 197,\n",
       " '都': 198,\n",
       " '不断': 199,\n",
       " '着': 200,\n",
       " '这次': 201,\n",
       " '恐外': 202,\n",
       " '从而': 203,\n",
       " '一种': 204,\n",
       " '边缘': 205,\n",
       " '革命': 206,\n",
       " '没有': 207,\n",
       " '也许': 208,\n",
       " '以': 209,\n",
       " '，': 210,\n",
       " '并': 211,\n",
       " '转折点': 212,\n",
       " '时机': 213,\n",
       " '看见': 214,\n",
       " '人': 215,\n",
       " '心情': 216,\n",
       " '里根': 217,\n",
       " '；': 218,\n",
       " '方面': 219,\n",
       " '有些': 220,\n",
       " '划时代': 221,\n",
       " '负': 222,\n",
       " '坚不可摧': 223,\n",
       " '年': 224,\n",
       " '优越性': 225,\n",
       " '雷曼': 226,\n",
       " '所': 227,\n",
       " '理想': 228,\n",
       " '制度': 229,\n",
       " '时期': 230,\n",
       " '-': 231,\n",
       " '结束': 232,\n",
       " '年份': 233,\n",
       " '结构性': 234,\n",
       " '象征': 235,\n",
       " '蔓延': 236,\n",
       " '可能': 237,\n",
       " '社会': 238,\n",
       " '对于': 239,\n",
       " '全球': 240,\n",
       " '会': 241,\n",
       " '把视': 242,\n",
       " '这一': 243,\n",
       " '如果': 244,\n",
       " '却是': 245,\n",
       " '捍卫': 246,\n",
       " '还是': 247,\n",
       " '分裂': 248,\n",
       " '而': 249,\n",
       " '其二': 250,\n",
       " '加深': 251,\n",
       " '即使': 252,\n",
       " '名义': 253,\n",
       " '我': 254,\n",
       " '兄弟': 255,\n",
       " '成果': 256,\n",
       " '机构': 257,\n",
       " '受到': 258,\n",
       " '带来': 259,\n",
       " '要么': 260,\n",
       " '些': 261,\n",
       " '这': 262,\n",
       " '下正': 263,\n",
       " '来': 264,\n",
       " '安心': 265,\n",
       " '仍': 266,\n",
       " '战胜': 267,\n",
       " '不是': 268,\n",
       " '取代': 269,\n",
       " '两者': 270,\n",
       " '?': 271,\n",
       " '趋势': 272,\n",
       " '包括': 273,\n",
       " '自由': 274,\n",
       " '鸿沟': 275,\n",
       " '苏联': 276,\n",
       " '表现': 277,\n",
       " '倒闭': 278,\n",
       " '也': 279,\n",
       " '良好': 280,\n",
       " '因为': 281,\n",
       " '美国': 282,\n",
       " '令人': 283,\n",
       " '上': 284,\n",
       " '自由市场': 285,\n",
       " '的话': 286,\n",
       " '：': 287,\n",
       " '很多': 288,\n",
       " '谨慎': 289,\n",
       " '和': 290,\n",
       " '避免': 291,\n",
       " '<pad>': 0,\n",
       " '<sos>': 292}"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['巴黎', '-', '随着', '经济危机', '不断']"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_words_list[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> At the start of the crisis , many people likened it to 1982 or 1973 , which was reassuring , because both dates refer to classical cyclical downturns . <EOS>\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(sentence):\n",
    "    indices = sentence_to_indices(sentence, chinese_vocab)\n",
    "    tensor = to_tensor(indices)\n",
    "    translation = translate(encoder, decoder, tensor, chinese_index_vocab, english_index_vocab)\n",
    "    return translation\n",
    "\n",
    "# 翻译示例\n",
    "print(translate_sentence(zh_words_list[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch size 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 编码器\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)  # Set batch_first to True\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)  # input: (batch_size, seq_len)\n",
    "        output, hidden = self.gru(embedded, hidden)  # No need to permute\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "# 解码器\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)  # Set batch_first to True\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input)  # input: (batch_size,seq)\n",
    "#         output = output.unsqueeze(1)  # Change to (batch_size, 1, hidden_size)\n",
    "        output = nn.functional.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)  # No need to permute\n",
    "        output = self.softmax(self.out(output.squeeze(1)))  # Squeeze to remove the sequence length dimension\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=10):\n",
    "    batch_size = input_tensor.size(0)\n",
    "    input_length = input_tensor.size(1)\n",
    "    target_length = target_tensor.size(1)\n",
    "\n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # Encode the input sequence batch\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "\n",
    "    # Prepare the initial decoder input (start with SOS tokens for each sequence in the batch)\n",
    "    decoder_input = torch.tensor([[SOS_token] * batch_size], device=device).transpose(0, 1)  # Shape: (batch_size, 1)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Assuming teacher forcing is not used here. If desired, it can be included with a certain probability.\n",
    "    for di in range(target_length):\n",
    "#         print(decoder_input.shape)\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.detach()  # Prepare next input\n",
    "#         print(decoder_output.shape,target_tensor[:, di].shape,decoder_input.shape)\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[:, di])\n",
    "\n",
    "        # Optionally, stop when all sequences in the batch reach EOS token\n",
    "        if (decoder_input == EOS_token).all():\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / (target_length * batch_size)  # Normalize loss by total number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_batch(batch_size, max_length, vocabulary_size):\n",
    "    \"\"\" Generate a random batch of sequences for training. \"\"\"\n",
    "    # Random sequences of random lengths\n",
    "    sequences = torch.randint(2, vocabulary_size, (batch_size, max_length)).to(device)\n",
    "    return sequences\n",
    "\n",
    "# Example of generating a batch\n",
    "input_tensor = generate_fake_batch(batch_size, max_length, input_size)\n",
    "target_tensor = generate_fake_batch(batch_size, max_length, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 4, 9, 8, 2, 7],\n",
       "        [6, 6, 7, 6, 8, 2, 5],\n",
       "        [9, 5, 6, 7, 3, 4, 4],\n",
       "        [4, 5, 5, 5, 7, 7, 5],\n",
       "        [3, 5, 6, 2, 5, 2, 2]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4680\n",
      "Epoch 2, Loss: 0.5614\n",
      "Epoch 3, Loss: 0.4901\n",
      "Epoch 4, Loss: 0.5791\n",
      "Epoch 5, Loss: 0.5343\n",
      "Epoch 6, Loss: 0.4939\n",
      "Epoch 7, Loss: 0.4654\n",
      "Epoch 8, Loss: 0.4840\n",
      "Epoch 9, Loss: 0.4593\n",
      "Epoch 10, Loss: 0.4287\n",
      "Average Loss: 0.4964\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(encoder, decoder, input_tensor, target_tensor):\n",
    "    loss_total = 0  # Track loss\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Generate new data for each epoch\n",
    "        input_tensor = generate_fake_batch(batch_size, max_length, input_size)\n",
    "        target_tensor = generate_fake_batch(batch_size, max_length, output_size)\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\n",
    "        loss_total += loss\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss:.4f}')\n",
    "\n",
    "    print(f'Average Loss: {loss_total / epochs:.4f}')\n",
    "\n",
    "# Call the training function\n",
    "train_epoch(encoder, decoder, input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch_len 版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden, input_lengths):\n",
    "        embedded = self.embedding(input)  # input: (batch_size, seq_len)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, hidden = self.gru(packed, hidden)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input)  # input: (batch_size, 1)\n",
    "        output = nn.functional.relu(output)\n",
    "#         print('input shape, output shape 111',input.shape,output.shape)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "#         print('input shape, output shape  222',input.shape,output.shape)\n",
    "        output = self.softmax(self.out(output.squeeze(1)))  # Make sure output is squeezed\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, input_lengths, output_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=10):\n",
    "    batch_size = input_tensor.size(0)\n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # 编码器处理输入\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden, input_lengths)\n",
    "\n",
    "    # 解码器的初始输入\n",
    "    decoder_input = torch.tensor([[SOS_token] * batch_size], device=device).transpose(0, 1)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # 解码器生成输出\n",
    "    for di in range(max(output_lengths)):  # 使用最大输出长度\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        decoder_input = decoder_output.topk(1)[1].detach()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            if di < output_lengths[i]:  # 只计算有效长度内的损失\n",
    "                loss += criterion(decoder_output[i], target_tensor[i, di])\n",
    "\n",
    "        if (decoder_input == EOS_token).all():\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / sum(output_lengths)  # 按有效输出长度归一化损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# 假设的词汇大小和隐藏层大小\n",
    "\n",
    "zhst = SentencesTool(zh_sentences)\n",
    "enst = SentencesTool(en_sentences)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = len(zhst.vocab)  # Example vocabulary size for encoder\n",
    "output_size = len(enst.vocab)  # Example vocabulary size for decoder\n",
    "\n",
    "# input_size = 10\n",
    "# output_size = 10\n",
    "hidden_size = 64\n",
    "\n",
    "# 特殊符号定义\n",
    "SOS_token = 0  # 句子开始符号\n",
    "EOS_token = 1  # 句子结束符号\n",
    "\n",
    "# 实例化模型\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(hidden_size, output_size).to(device)\n",
    "\n",
    "# 优化器\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.01)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.01)\n",
    "\n",
    "# 损失函数\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# 创建一些虚拟数据\n",
    "def generate_batch(batch_size):\n",
    "    max_length = 10\n",
    "    input_tensor = torch.LongTensor(batch_size, max_length).random_(2, input_size).to(device)\n",
    "    target_tensor = torch.LongTensor(batch_size, max_length).random_(2, output_size).to(device)\n",
    "    input_lengths = torch.LongTensor([random.randint(5, max_length) for _ in range(batch_size)]).to(device)\n",
    "    target_lengths = torch.LongTensor([random.randint(5, max_length) for _ in range(batch_size)]).to(device)\n",
    "    return input_tensor, target_tensor, input_lengths, target_lengths\n",
    "\n",
    "batch_size = 5\n",
    "# input_tensor, target_tensor, input_lengths, target_lengths = generate_batch(batch_size)\n",
    "input_tensor,input_lengths = zhst.tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tensor, target_lengths = enst.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor,input_lengths = input_tensor[:batch_size],input_lengths[:batch_size]\n",
    "target_tensor, target_lengths = target_tensor[:batch_size], target_lengths[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[67735,  9926, 29499,  ...,     0,     0,     0],\n",
       "         [78663, 29206, 60089,  ...,     0,     0,     0],\n",
       "         [26019, 59250,  4319,  ...,     0,     0,     0],\n",
       "         [ 8576,    22, 79893,  ...,     0,     0,     0],\n",
       "         [83817, 79893, 60604,  ...,     0,     0,     0]]),\n",
       " tensor([ 7, 30, 38, 45, 24]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor,input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.06798267364502\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "loss = train(input_tensor, target_tensor, input_lengths, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "\n",
    "# 损失函数\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.923974990844727\n",
      "Loss: 10.897537231445312\n",
      "Loss: 10.850257873535156\n",
      "Loss: 10.814164161682129\n",
      "Loss: 10.756314277648926\n",
      "Loss: 10.675296783447266\n",
      "Loss: 10.63945484161377\n",
      "Loss: 10.584932327270508\n",
      "Loss: 10.485472679138184\n",
      "Loss: 10.402478218078613\n",
      "Loss: 10.396851539611816\n",
      "Loss: 10.326165199279785\n",
      "Loss: 10.166382789611816\n",
      "Loss: 10.086796760559082\n",
      "Loss: 10.05424690246582\n",
      "Loss: 9.913763046264648\n",
      "Loss: 9.758529663085938\n",
      "Loss: 9.705838203430176\n",
      "Loss: 9.61233901977539\n",
      "Loss: 9.552497863769531\n",
      "Loss: 9.3374605178833\n",
      "Loss: 9.263070106506348\n",
      "Loss: 9.13524055480957\n",
      "Loss: 9.076848983764648\n",
      "Loss: 9.035099029541016\n",
      "Loss: 9.034539222717285\n",
      "Loss: 8.930573463439941\n",
      "Loss: 8.751213073730469\n",
      "Loss: 8.60395336151123\n",
      "Loss: 8.698591232299805\n",
      "Loss: 8.495880126953125\n",
      "Loss: 8.483183860778809\n",
      "Loss: 8.212947845458984\n",
      "Loss: 8.255971908569336\n",
      "Loss: 8.134003639221191\n",
      "Loss: 8.087899208068848\n",
      "Loss: 7.9865403175354\n",
      "Loss: 8.024480819702148\n",
      "Loss: 7.953485012054443\n",
      "Loss: 8.006179809570312\n",
      "Loss: 8.036958694458008\n",
      "Loss: 7.885465621948242\n",
      "Loss: 7.8766584396362305\n",
      "Loss: 7.705907344818115\n",
      "Loss: 7.635448455810547\n",
      "Loss: 7.612295150756836\n",
      "Loss: 7.475311756134033\n",
      "Loss: 7.421541213989258\n",
      "Loss: 7.4391279220581055\n",
      "Loss: 7.468105316162109\n",
      "Loss: 7.649195194244385\n",
      "Loss: 7.496215343475342\n",
      "Loss: 7.453811168670654\n",
      "Loss: 7.277817249298096\n",
      "Loss: 7.144651412963867\n",
      "Loss: 7.486551761627197\n",
      "Loss: 7.035012722015381\n",
      "Loss: 7.025704860687256\n",
      "Loss: 7.207031726837158\n",
      "Loss: 7.153660774230957\n",
      "Loss: 6.982382297515869\n",
      "Loss: 7.341049671173096\n",
      "Loss: 6.939418315887451\n",
      "Loss: 6.888406753540039\n",
      "Loss: 7.073810577392578\n",
      "Loss: 6.945745944976807\n",
      "Loss: 7.007461071014404\n",
      "Loss: 7.215451717376709\n",
      "Loss: 7.074220180511475\n",
      "Loss: 7.102136135101318\n",
      "Loss: 7.120091438293457\n",
      "Loss: 7.25676965713501\n",
      "Loss: 7.319962978363037\n",
      "Loss: 6.943652629852295\n",
      "Loss: 7.026796817779541\n",
      "Loss: 7.0667033195495605\n",
      "Loss: 7.166313171386719\n",
      "Loss: 6.994962215423584\n",
      "Loss: 7.031814098358154\n",
      "Loss: 7.200904369354248\n",
      "Loss: 7.45950984954834\n",
      "Loss: 6.923352241516113\n",
      "Loss: 6.985659122467041\n",
      "Loss: 7.00557279586792\n",
      "Loss: 7.043163299560547\n",
      "Loss: 7.202639579772949\n",
      "Loss: 7.236764907836914\n",
      "Loss: 6.914108753204346\n",
      "Loss: 6.857198715209961\n",
      "Loss: 7.190764904022217\n",
      "Loss: 7.6671905517578125\n",
      "Loss: 7.324923038482666\n",
      "Loss: 6.933794975280762\n",
      "Loss: 6.781205654144287\n",
      "Loss: 7.053581714630127\n",
      "Loss: 6.917036533355713\n",
      "Loss: 6.8569560050964355\n",
      "Loss: 6.761254787445068\n",
      "Loss: 7.030201435089111\n",
      "Loss: 6.9257378578186035\n",
      "Loss: 6.989851474761963\n",
      "Loss: 6.7373433113098145\n",
      "Loss: 6.830927848815918\n",
      "Loss: 6.974109649658203\n",
      "Loss: 6.644118309020996\n",
      "Loss: 6.80120325088501\n",
      "Loss: 6.914608955383301\n",
      "Loss: 7.098572254180908\n",
      "Loss: 6.833810806274414\n",
      "Loss: 6.970024585723877\n",
      "Loss: 6.945261001586914\n",
      "Loss: 6.900210380554199\n",
      "Loss: 6.983915328979492\n",
      "Loss: 6.7872467041015625\n",
      "Loss: 6.977029800415039\n",
      "Loss: 6.938692569732666\n",
      "Loss: 6.75058126449585\n",
      "Loss: 6.741364479064941\n",
      "Loss: 6.992222309112549\n",
      "Loss: 6.735498905181885\n",
      "Loss: 6.871021270751953\n",
      "Loss: 7.088148593902588\n",
      "Loss: 6.908496379852295\n",
      "Loss: 6.944642066955566\n",
      "Loss: 6.768624305725098\n",
      "Loss: 6.682642459869385\n",
      "Loss: 6.889540672302246\n",
      "Loss: 6.792651653289795\n",
      "Loss: 6.799931049346924\n",
      "Loss: 6.981786251068115\n",
      "Loss: 6.688193321228027\n",
      "Loss: 6.967040061950684\n",
      "Loss: 6.66495943069458\n",
      "Loss: 6.9843339920043945\n",
      "Loss: 6.895667552947998\n",
      "Loss: 6.747650146484375\n",
      "Loss: 6.76241397857666\n",
      "Loss: 6.6049299240112305\n",
      "Loss: 6.691327095031738\n",
      "Loss: 6.816041946411133\n",
      "Loss: 6.627053737640381\n",
      "Loss: 6.630494117736816\n",
      "Loss: 6.734375\n",
      "Loss: 7.006959915161133\n",
      "Loss: 6.690720558166504\n",
      "Loss: 6.950153827667236\n",
      "Loss: 7.058995723724365\n",
      "Loss: 6.8534369468688965\n",
      "Loss: 6.862888336181641\n",
      "Loss: 6.892054080963135\n",
      "Loss: 7.01516056060791\n",
      "Loss: 6.789309024810791\n",
      "Loss: 6.797585487365723\n",
      "Loss: 6.887760639190674\n",
      "Loss: 6.8271403312683105\n",
      "Loss: 6.9366230964660645\n",
      "Loss: 6.876476764678955\n",
      "Loss: 7.084937572479248\n",
      "Loss: 6.8243584632873535\n",
      "Loss: 6.879753112792969\n",
      "Loss: 6.897872447967529\n",
      "Loss: 7.125858783721924\n",
      "Loss: 6.966806888580322\n",
      "Loss: 7.039342880249023\n",
      "Loss: 7.191298007965088\n",
      "Loss: 7.02962589263916\n",
      "Loss: 7.032589435577393\n",
      "Loss: 6.723976135253906\n",
      "Loss: 7.060603141784668\n",
      "Loss: 6.959167003631592\n",
      "Loss: 6.965221881866455\n",
      "Loss: 6.817179203033447\n",
      "Loss: 7.002503871917725\n",
      "Loss: 6.701823711395264\n",
      "Loss: 6.759254455566406\n",
      "Loss: 6.775633811950684\n",
      "Loss: 6.987839221954346\n",
      "Loss: 6.739044666290283\n",
      "Loss: 6.7861328125\n",
      "Loss: 6.865760803222656\n",
      "Loss: 6.844668865203857\n",
      "Loss: 6.778494358062744\n",
      "Loss: 7.129870414733887\n",
      "Loss: 6.970877170562744\n",
      "Loss: 6.753614902496338\n",
      "Loss: 6.713616847991943\n",
      "Loss: 7.005466461181641\n",
      "Loss: 7.006317138671875\n",
      "Loss: 6.93396520614624\n",
      "Loss: 6.9297356605529785\n",
      "Loss: 6.960871696472168\n",
      "Loss: 6.864530086517334\n",
      "Loss: 6.859135150909424\n",
      "Loss: 6.902398586273193\n",
      "Loss: 6.715251445770264\n",
      "Loss: 6.968510627746582\n",
      "Loss: 6.818787574768066\n",
      "Loss: 6.96037483215332\n",
      "Loss: 7.060079097747803\n",
      "Loss: 7.0221266746521\n",
      "Loss: 6.875200271606445\n",
      "Loss: 6.871399402618408\n",
      "Loss: 7.001886367797852\n",
      "Loss: 6.770868301391602\n",
      "Loss: 6.841246604919434\n",
      "Loss: 7.2273054122924805\n",
      "Loss: 7.031925678253174\n",
      "Loss: 7.030909538269043\n",
      "Loss: 7.164377212524414\n",
      "Loss: 7.0825724601745605\n",
      "Loss: 6.999348163604736\n",
      "Loss: 6.956961631774902\n",
      "Loss: 6.871494293212891\n",
      "Loss: 6.884620666503906\n",
      "Loss: 6.912261486053467\n",
      "Loss: 7.120934963226318\n",
      "Loss: 6.992584228515625\n",
      "Loss: 6.988984107971191\n",
      "Loss: 6.969282150268555\n",
      "Loss: 7.16252326965332\n",
      "Loss: 6.950803279876709\n",
      "Loss: 6.9266180992126465\n",
      "Loss: 6.97451639175415\n",
      "Loss: 6.998805999755859\n",
      "Loss: 6.972691535949707\n",
      "Loss: 6.957371234893799\n",
      "Loss: 6.839820861816406\n",
      "Loss: 6.912055492401123\n",
      "Loss: 6.916431427001953\n",
      "Loss: 6.881305694580078\n",
      "Loss: 7.069824695587158\n",
      "Loss: 6.789654731750488\n",
      "Loss: 6.719952583312988\n",
      "Loss: 7.03770112991333\n",
      "Loss: 6.683169841766357\n",
      "Loss: 6.972473621368408\n",
      "Loss: 6.810249328613281\n",
      "Loss: 6.95210599899292\n",
      "Loss: 6.8582072257995605\n",
      "Loss: 6.945087909698486\n",
      "Loss: 6.8504862785339355\n",
      "Loss: 6.8096418380737305\n",
      "Loss: 7.028639316558838\n",
      "Loss: 7.063990592956543\n",
      "Loss: 6.875901222229004\n",
      "Loss: 7.337759494781494\n",
      "Loss: 7.00883150100708\n",
      "Loss: 6.877220630645752\n",
      "Loss: 6.859066009521484\n",
      "Loss: 6.86386251449585\n",
      "Loss: 6.8308424949646\n",
      "Loss: 6.849370956420898\n",
      "Loss: 6.838407039642334\n",
      "Loss: 6.939724445343018\n",
      "Loss: 6.5612945556640625\n",
      "Loss: 6.5944037437438965\n",
      "Loss: 6.588764190673828\n",
      "Loss: 6.74640417098999\n",
      "Loss: 6.863900184631348\n",
      "Loss: 7.03604793548584\n",
      "Loss: 6.9783735275268555\n",
      "Loss: 7.105767250061035\n",
      "Loss: 6.927604675292969\n",
      "Loss: 6.725843906402588\n",
      "Loss: 6.68897819519043\n",
      "Loss: 6.709108829498291\n",
      "Loss: 6.5297675132751465\n",
      "Loss: 6.595126628875732\n",
      "Loss: 6.752957344055176\n",
      "Loss: 7.029728889465332\n",
      "Loss: 6.8356614112854\n",
      "Loss: 6.880924224853516\n",
      "Loss: 6.8086748123168945\n",
      "Loss: 6.796645641326904\n",
      "Loss: 6.934319496154785\n",
      "Loss: 6.732696056365967\n",
      "Loss: 6.8164825439453125\n",
      "Loss: 7.038717746734619\n",
      "Loss: 6.798511981964111\n",
      "Loss: 7.0176005363464355\n",
      "Loss: 6.787563323974609\n",
      "Loss: 6.933231353759766\n",
      "Loss: 6.770644187927246\n",
      "Loss: 6.666913032531738\n",
      "Loss: 7.002479553222656\n",
      "Loss: 6.810649871826172\n",
      "Loss: 6.681879997253418\n",
      "Loss: 6.792582035064697\n",
      "Loss: 6.78603982925415\n",
      "Loss: 6.711694240570068\n",
      "Loss: 6.786628723144531\n",
      "Loss: 6.921765327453613\n",
      "Loss: 6.99555778503418\n",
      "Loss: 6.8779425621032715\n",
      "Loss: 6.90196418762207\n",
      "Loss: 6.8808064460754395\n",
      "Loss: 7.040275573730469\n",
      "Loss: 6.804860591888428\n",
      "Loss: 7.0028533935546875\n",
      "Loss: 6.860586166381836\n",
      "Loss: 6.878561019897461\n",
      "Loss: 6.7778239250183105\n",
      "Loss: 6.556471347808838\n",
      "Loss: 6.534389019012451\n",
      "Loss: 6.846768379211426\n",
      "Loss: 6.983351707458496\n",
      "Loss: 6.765644073486328\n",
      "Loss: 7.012015342712402\n",
      "Loss: 6.929758071899414\n",
      "Loss: 6.707276344299316\n",
      "Loss: 6.727415084838867\n",
      "Loss: 7.005497932434082\n",
      "Loss: 6.694799900054932\n",
      "Loss: 6.812293529510498\n",
      "Loss: 6.939610481262207\n",
      "Loss: 6.696147441864014\n",
      "Loss: 6.632083415985107\n",
      "Loss: 7.092314720153809\n",
      "Loss: 6.697593688964844\n",
      "Loss: 6.7401556968688965\n",
      "Loss: 6.799527645111084\n",
      "Loss: 7.098670482635498\n",
      "Loss: 6.585844993591309\n",
      "Loss: 6.665514945983887\n",
      "Loss: 6.934447765350342\n",
      "Loss: 6.842631816864014\n",
      "Loss: 6.775399684906006\n",
      "Loss: 6.910675048828125\n",
      "Loss: 6.7279438972473145\n",
      "Loss: 6.634893417358398\n",
      "Loss: 6.643791675567627\n",
      "Loss: 6.852657318115234\n",
      "Loss: 7.156280994415283\n",
      "Loss: 7.005834102630615\n",
      "Loss: 6.955931186676025\n",
      "Loss: 7.026379585266113\n",
      "Loss: 7.032252311706543\n",
      "Loss: 6.765708923339844\n",
      "Loss: 6.979992389678955\n",
      "Loss: 6.871174335479736\n",
      "Loss: 6.835716247558594\n",
      "Loss: 6.796422958374023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.863436698913574\n",
      "Loss: 7.00215482711792\n",
      "Loss: 6.89874792098999\n",
      "Loss: 6.722559452056885\n",
      "Loss: 6.875668048858643\n",
      "Loss: 6.700798034667969\n",
      "Loss: 6.934818267822266\n",
      "Loss: 6.867824077606201\n",
      "Loss: 7.1151957511901855\n",
      "Loss: 7.024426460266113\n",
      "Loss: 6.804614543914795\n",
      "Loss: 6.939107418060303\n",
      "Loss: 7.069117546081543\n",
      "Loss: 6.8576154708862305\n",
      "Loss: 7.067151069641113\n",
      "Loss: 7.199041843414307\n",
      "Loss: 6.68327522277832\n",
      "Loss: 6.804258823394775\n",
      "Loss: 6.976616382598877\n",
      "Loss: 6.829182147979736\n",
      "Loss: 6.629719257354736\n",
      "Loss: 6.807776927947998\n",
      "Loss: 6.732226371765137\n",
      "Loss: 6.904170036315918\n",
      "Loss: 6.570226669311523\n",
      "Loss: 6.585124492645264\n",
      "Loss: 6.834481239318848\n",
      "Loss: 6.949233055114746\n",
      "Loss: 6.932121753692627\n",
      "Loss: 7.216623783111572\n",
      "Loss: 7.120263576507568\n",
      "Loss: 6.98634147644043\n",
      "Loss: 6.923015117645264\n",
      "Loss: 6.771229267120361\n",
      "Loss: 6.722348690032959\n",
      "Loss: 7.030975341796875\n",
      "Loss: 6.912806034088135\n",
      "Loss: 6.79537296295166\n",
      "Loss: 6.8603315353393555\n",
      "Loss: 6.6754021644592285\n",
      "Loss: 6.933804512023926\n",
      "Loss: 7.107382774353027\n",
      "Loss: 6.921473979949951\n",
      "Loss: 6.905823707580566\n",
      "Loss: 6.783603668212891\n",
      "Loss: 6.826406002044678\n",
      "Loss: 6.84654426574707\n",
      "Loss: 6.701620101928711\n",
      "Loss: 6.802112102508545\n",
      "Loss: 6.989665985107422\n",
      "Loss: 7.2440385818481445\n",
      "Loss: 6.839848041534424\n",
      "Loss: 6.81215238571167\n",
      "Loss: 6.591847896575928\n",
      "Loss: 6.520146369934082\n",
      "Loss: 6.74036979675293\n",
      "Loss: 6.825906276702881\n",
      "Loss: 6.615962982177734\n",
      "Loss: 6.484766960144043\n",
      "Loss: 6.504357814788818\n",
      "Loss: 6.609489917755127\n",
      "Loss: 6.3235063552856445\n",
      "Loss: 6.464426517486572\n",
      "Loss: 6.556926250457764\n",
      "Loss: 6.749098777770996\n",
      "Loss: 6.643263339996338\n",
      "Loss: 6.507713317871094\n",
      "Loss: 6.6072893142700195\n",
      "Loss: 6.6505279541015625\n",
      "Loss: 6.524017810821533\n",
      "Loss: 6.810734748840332\n",
      "Loss: 6.8283586502075195\n",
      "Loss: 6.948611259460449\n",
      "Loss: 6.574618339538574\n",
      "Loss: 6.728217124938965\n",
      "Loss: 6.573144435882568\n",
      "Loss: 6.828935146331787\n",
      "Loss: 6.6164984703063965\n",
      "Loss: 6.390063762664795\n",
      "Loss: 6.592980861663818\n",
      "Loss: 6.657528400421143\n",
      "Loss: 6.561794281005859\n",
      "Loss: 6.696389198303223\n",
      "Loss: 6.774197101593018\n",
      "Loss: 7.127905368804932\n",
      "Loss: 9.537344932556152\n",
      "Loss: 6.964925289154053\n",
      "Loss: 6.796683311462402\n",
      "Loss: 6.588413715362549\n",
      "Loss: 6.972949981689453\n",
      "Loss: 6.77044677734375\n",
      "Loss: 6.640410423278809\n",
      "Loss: 6.8596649169921875\n",
      "Loss: 6.96403694152832\n",
      "Loss: 6.819210529327393\n",
      "Loss: 6.8211517333984375\n",
      "Loss: 7.042184352874756\n",
      "Loss: 6.843915939331055\n",
      "Loss: 6.601134777069092\n",
      "Loss: 7.070036888122559\n",
      "Loss: 6.790276050567627\n",
      "Loss: 6.858741760253906\n",
      "Loss: 7.190745830535889\n",
      "Loss: 6.921437740325928\n",
      "Loss: 6.880340099334717\n",
      "Loss: 6.90363883972168\n",
      "Loss: 6.767847061157227\n",
      "Loss: 7.2106757164001465\n",
      "Loss: 6.902472496032715\n",
      "Loss: 6.782558441162109\n",
      "Loss: 6.739682197570801\n",
      "Loss: 6.863371849060059\n",
      "Loss: 6.714180946350098\n",
      "Loss: 7.047944068908691\n",
      "Loss: 6.997145652770996\n",
      "Loss: 6.712059020996094\n",
      "Loss: 6.802025318145752\n",
      "Loss: 6.83487606048584\n",
      "Loss: 6.764069080352783\n",
      "Loss: 6.987274169921875\n",
      "Loss: 6.666154384613037\n",
      "Loss: 6.667552471160889\n",
      "Loss: 6.679975509643555\n",
      "Loss: 6.731605052947998\n",
      "Loss: 6.80570650100708\n",
      "Loss: 6.691814422607422\n",
      "Loss: 6.51728630065918\n",
      "Loss: 6.766766548156738\n",
      "Loss: 7.017385959625244\n",
      "Loss: 6.659348011016846\n",
      "Loss: 6.830977439880371\n",
      "Loss: 6.2020087242126465\n",
      "Loss: 6.749364376068115\n",
      "Loss: 5.912388801574707\n",
      "Loss: 6.772276878356934\n",
      "Loss: 6.896872520446777\n",
      "Loss: 7.058535099029541\n",
      "Loss: 7.023539066314697\n",
      "Loss: 7.130076885223389\n",
      "Loss: 6.601113319396973\n",
      "Loss: 6.851362228393555\n",
      "Loss: 6.538093566894531\n",
      "Loss: 6.799960136413574\n",
      "Loss: 6.819989204406738\n",
      "Loss: 6.758702278137207\n",
      "Loss: 6.786041259765625\n",
      "Loss: 6.866711616516113\n",
      "Loss: 6.988124847412109\n",
      "Loss: 6.832971572875977\n",
      "Loss: 6.9512457847595215\n",
      "Loss: 6.651339054107666\n",
      "Loss: 6.8019118309021\n",
      "Loss: 6.810113906860352\n",
      "Loss: 6.643034934997559\n",
      "Loss: 6.607907772064209\n",
      "Loss: 6.889336109161377\n",
      "Loss: 6.7585296630859375\n",
      "Loss: 6.798298358917236\n",
      "Loss: 6.709831237792969\n",
      "Loss: 6.807545185089111\n",
      "Loss: 6.625781059265137\n",
      "Loss: 6.696141242980957\n",
      "Loss: 6.807251453399658\n",
      "Loss: 7.041187763214111\n",
      "Loss: 6.8859028816223145\n",
      "Loss: 7.159752368927002\n",
      "Loss: 6.770997524261475\n",
      "Loss: 7.103782653808594\n",
      "Loss: 6.615766525268555\n",
      "Loss: 6.823542594909668\n",
      "Loss: 6.79833459854126\n",
      "Loss: 7.032292366027832\n",
      "Loss: 6.625542640686035\n",
      "Loss: 6.518997669219971\n",
      "Loss: 6.576460838317871\n",
      "Loss: 6.530659198760986\n",
      "Loss: 6.605645656585693\n",
      "Loss: 6.619543552398682\n",
      "Loss: 6.877299785614014\n",
      "Loss: 6.554423809051514\n",
      "Loss: 6.719064235687256\n",
      "Loss: 6.832681655883789\n",
      "Loss: 6.882693290710449\n",
      "Loss: 6.693018436431885\n",
      "Loss: 6.50090217590332\n",
      "Loss: 6.5580573081970215\n",
      "Loss: 6.571971893310547\n",
      "Loss: 6.69722318649292\n",
      "Loss: 6.759063243865967\n",
      "Loss: 6.5372443199157715\n",
      "Loss: 6.737401008605957\n",
      "Loss: 6.543741226196289\n",
      "Loss: 6.431167125701904\n",
      "Loss: 6.382967948913574\n",
      "Loss: 6.514171600341797\n",
      "Loss: 6.625294208526611\n",
      "Loss: 6.568277835845947\n",
      "Loss: 6.457089900970459\n",
      "Loss: 6.584067344665527\n",
      "Loss: 6.617727279663086\n",
      "Loss: 6.473334789276123\n",
      "Loss: 6.489832878112793\n",
      "Loss: 6.335850238800049\n",
      "Loss: 6.4598069190979\n",
      "Loss: 6.612552642822266\n",
      "Loss: 6.652398109436035\n",
      "Loss: 6.535135746002197\n",
      "Loss: 6.316739082336426\n",
      "Loss: 6.497451305389404\n",
      "Loss: 6.607702732086182\n",
      "Loss: 6.62448263168335\n",
      "Loss: 6.480154991149902\n",
      "Loss: 6.618228435516357\n",
      "Loss: 6.586132049560547\n",
      "Loss: 6.516565799713135\n",
      "Loss: 6.502763748168945\n",
      "Loss: 6.659230709075928\n",
      "Loss: 6.471872806549072\n",
      "Loss: 6.740498065948486\n",
      "Loss: 6.310370922088623\n",
      "Loss: 6.454903602600098\n",
      "Loss: 6.476558208465576\n",
      "Loss: 6.673410892486572\n",
      "Loss: 7.013547420501709\n",
      "Loss: 6.797743320465088\n",
      "Loss: 6.471860885620117\n",
      "Loss: 6.568481922149658\n",
      "Loss: 6.512193202972412\n",
      "Loss: 6.436952114105225\n",
      "Loss: 6.623793125152588\n",
      "Loss: 6.5269880294799805\n",
      "Loss: 7.070071697235107\n",
      "Loss: 6.876200199127197\n",
      "Loss: 7.059326171875\n",
      "Loss: 6.701146602630615\n",
      "Loss: 6.841796398162842\n",
      "Loss: 6.431242942810059\n",
      "Loss: 6.658455848693848\n",
      "Loss: 6.701870441436768\n",
      "Loss: 6.738444805145264\n",
      "Loss: 6.655545234680176\n",
      "Loss: 6.907138824462891\n",
      "Loss: 6.787407398223877\n",
      "Loss: 6.642495155334473\n",
      "Loss: 6.776200294494629\n",
      "Loss: 6.706855773925781\n",
      "Loss: 6.34876012802124\n",
      "Loss: 6.940186977386475\n",
      "Loss: 6.3991875648498535\n",
      "Loss: 6.691919803619385\n",
      "Loss: 6.531247615814209\n",
      "Loss: 6.945283889770508\n",
      "Loss: 6.898504257202148\n",
      "Loss: 6.738467693328857\n",
      "Loss: 6.787909507751465\n",
      "Loss: 6.511524677276611\n",
      "Loss: 6.684309959411621\n",
      "Loss: 6.441304683685303\n",
      "Loss: 6.479162693023682\n",
      "Loss: 6.571532249450684\n",
      "Loss: 7.050675392150879\n",
      "Loss: 6.892876148223877\n",
      "Loss: 6.575751781463623\n",
      "Loss: 6.386407852172852\n",
      "Loss: 6.759076118469238\n",
      "Loss: 6.381434917449951\n",
      "Loss: 6.673488140106201\n",
      "Loss: 6.427220821380615\n",
      "Loss: 6.829935073852539\n",
      "Loss: 6.494718074798584\n",
      "Loss: 6.59435510635376\n",
      "Loss: 6.335287094116211\n",
      "Loss: 6.8094377517700195\n",
      "Loss: 6.331521987915039\n",
      "Loss: 6.505726337432861\n",
      "Loss: 6.649287700653076\n",
      "Loss: 6.714035987854004\n",
      "Loss: 6.415852069854736\n",
      "Loss: 6.256265640258789\n",
      "Loss: 6.659126281738281\n",
      "Loss: 6.581993103027344\n",
      "Loss: 6.516238689422607\n",
      "Loss: 6.417453765869141\n",
      "Loss: 6.354693412780762\n",
      "Loss: 6.476785182952881\n",
      "Loss: 6.8663649559021\n",
      "Loss: 6.8561906814575195\n",
      "Loss: 6.644654273986816\n",
      "Loss: 6.693287372589111\n",
      "Loss: 6.866488933563232\n",
      "Loss: 6.9135894775390625\n",
      "Loss: 6.847236633300781\n"
     ]
    }
   ],
   "source": [
    "for e in range(100):\n",
    "    b = 64\n",
    "    for i in range(len(zhst.tensors[0])//b):\n",
    "        input_tensor,input_lengths = zhst.tensors[0][i*b:(i+1)*b],zhst.tensors[1][i*b:(i+1)*b]\n",
    "        target_tensor, target_lengths = enst.tensors[0][i*b:(i+1)*b],enst.tensors[1][i*b:(i+1)*b]\n",
    "        loss = train(input_tensor, target_tensor, input_lengths, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_path = '/Users/liuchu/linear_algebra_strang/translate/中英翻译数据集/train/news-commentary-v13.zh-en.en'\n",
    "zh_path = '/Users/liuchu/linear_algebra_strang/translate/中英翻译数据集/train/news-commentary-v13.zh-en.zh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_en_sentences():\n",
    "    arr = []\n",
    "    with open(en_path,'r') as f:\n",
    "        count = 0\n",
    "        for line in f.readlines():\n",
    "            arr.append(line.strip())\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zh_sentences():\n",
    "    arr = []\n",
    "    with open(zh_path,'r') as f:\n",
    "        count = 0\n",
    "        for line in f.readlines():\n",
    "            arr.append(line.strip().replace(' ',''))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentences = get_en_sentences()\n",
    "zh_sentences = get_zh_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sentences = en_sentences\n",
    "target_sentences = zh_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 编码器\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "# 解码器\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = nn.functional.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=10):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS_token是起始符号的标记\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:  # EOS_token是结束符号的标记\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorFromSentence(vocab, sentence):\n",
    "    indices = [vocab[char] for char in sentence if char in vocab]  # Ensure char is in vocab\n",
    "    indices.append(vocab['<eos>'])  # Append EOS token\n",
    "    return torch.tensor(indices, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def translate_sentence(sentence, encoder, decoder, input_vocab, output_vocab):\n",
    "    input_tensor = tensorFromSentence(input_vocab, sentence)\n",
    "    translated_sentence = translate(encoder, decoder, input_tensor, input_vocab, output_vocab)\n",
    "    return translated_sentence\n",
    "\n",
    "def translate(encoder, decoder, input_tensor, input_vocab, output_vocab, max_length=100):\n",
    "    with torch.no_grad():\n",
    "        input_length = input_tensor.size(0)\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        # Forward pass through encoder\n",
    "        for ei in range(input_length):\n",
    "            _, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "        # Decoder's first input is SOS token\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_word = output_vocab[topi.item()]\n",
    "                decoded_words.append(decoded_word)\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return ' '.join(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设语料数据如下，这里我们使用了非常简单的例子\n",
    "chinese_sentences = zh_sentences[:20]\n",
    "english_sentences = en_sentences[:20]\n",
    "\n",
    "# 为了简化问题，我们不使用词嵌入，而是直接根据字母或汉字进行索引\n",
    "def create_dict(sentences):\n",
    "    vocab = set(''.join(sentences))\n",
    "    char_to_index = {char: i+1 for i, char in enumerate(vocab)}  # +1因为我们留出0作为padding\n",
    "    char_to_index['<pad>'] = 0\n",
    "    index_to_char = {i: char for char, i in char_to_index.items()}\n",
    "    return char_to_index, index_to_char\n",
    "\n",
    "chinese_vocab, chinese_index_vocab = create_dict(chinese_sentences)\n",
    "chinese_vocab['<sos>'] = max(chinese_vocab.values()) + 1\n",
    "chinese_index_vocab[chinese_vocab['<sos>']] = '<sos>'\n",
    "\n",
    "english_vocab, english_index_vocab = create_dict(english_sentences)\n",
    "\n",
    "# 将句子转换为索引\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab[char] for char in sentence]\n",
    "\n",
    "chinese_data = [sentence_to_indices(sentence, chinese_vocab) for sentence in chinese_sentences]\n",
    "english_data = [sentence_to_indices(sentence, english_vocab) for sentence in english_sentences]\n",
    "\n",
    "# 增加开始和结束标记\n",
    "\n",
    "SOS_token = english_vocab['<sos>'] = max(english_vocab.values()) + 1\n",
    "EOS_token = english_vocab['<eos>'] = max(english_vocab.values()) + 1\n",
    "english_index_vocab[SOS_token] = '<sos>'\n",
    "english_index_vocab[EOS_token] = '<eos>'\n",
    "english_data = [[SOS_token] + sentence + [EOS_token] for sentence in english_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 61)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_index_vocab),len(english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chinese_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据转换为torch张量\n",
    "def to_tensor(data):\n",
    "    return torch.tensor(data, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "chinese_tensors = [to_tensor(data) for data in chinese_data]\n",
    "english_tensors = [to_tensor(data) for data in english_data]\n",
    "\n",
    "# 训练参数设置\n",
    "num_epochs = 100\n",
    "\n",
    "encoder = Encoder(len(chinese_vocab), 256).to(device)\n",
    "decoder = Decoder(256, len(english_vocab)).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.2947\n",
      "Epoch 20, Loss: 1.2528\n",
      "Epoch 30, Loss: 1.2216\n",
      "Epoch 40, Loss: 1.1722\n",
      "Epoch 50, Loss: 1.1268\n",
      "Epoch 60, Loss: 1.1587\n"
     ]
    }
   ],
   "source": [
    "# 训练循环\n",
    "for epoch in range(num_epochs+200):\n",
    "    total_loss = 0\n",
    "    for ch_tensor, en_tensor in zip(chinese_tensors, english_tensors):\n",
    "        loss = train(ch_tensor, en_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        total_loss += loss\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(chinese_sentences):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1929年还是1989年?',\n",
       " '巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。',\n",
       " '一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为这两段时期意味着典型的周期性衰退。',\n",
       " '如今人们的心情却是沉重多了，许多人开始把这次危机与1929年和1931年相比，即使一些国家政府的表现仍然似乎把视目前的情况为是典型的而看见的衰退。',\n",
       " '目前的趋势是，要么是过度的克制（欧洲），要么是努力的扩展（美国）。',\n",
       " '欧洲在避免债务和捍卫欧元的名义下正变得谨慎，而美国已经在许多方面行动起来，以利用这一理想的时机来实行急需的结构性改革。',\n",
       " '然而，作为地域战略学家，无论是从政治意义还是从经济意义上，让我自然想到的年份是1989年。',\n",
       " '当然，雷曼兄弟公司的倒闭和柏林墙的倒塌没有任何关系。',\n",
       " '事实上，从表面上看，两者似乎是完全是相反的：一个是象征着压抑和人为分裂的柏林墙的倒塌，而另一个是看似坚不可摧的并令人安心的金融资本主义机构的倒塌。',\n",
       " '然而，和1989年一样，2008-2009年很可能也能被视为一个划时代的改变，其带来的发人深省的后果将在几十年后仍能让我们感受得到。']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1929 or 1989?',\n",
       " 'PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.',\n",
       " 'At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.',\n",
       " 'Today, the mood is much grimmer, with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.',\n",
       " 'The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).',\n",
       " 'Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.',\n",
       " 'For geo-strategists, however, the year that naturally comes to mind, in both politics and economics, is 1989.',\n",
       " 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       " 'Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and reassuring institution of financial capitalism.',\n",
       " 'Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose unfolding consequences will be felt for decades.']"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o r   s c a t ,   h i s i s e e i s e e e   e i e e   i e r s , e   ,   e   e   t   e     t     t                           c     c   c a     c n c i   n s n a i a i n i a i l s n i i . n i . l . <EOS>\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(sentence):\n",
    "    indices = sentence_to_indices(sentence, chinese_vocab)\n",
    "    tensor = to_tensor(indices)\n",
    "    translation = translate(encoder, decoder, tensor, chinese_index_vocab, english_index_vocab)\n",
    "    return translation\n",
    "\n",
    "# 翻译示例\n",
    "print(translate_sentence('事实上，从表面上看'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

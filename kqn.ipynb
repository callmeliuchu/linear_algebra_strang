{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7877531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.nn import Module, Embedding, LSTM, Linear, Dropout, Sequential, ReLU\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "class KQN(Module):\n",
    "    def __init__(self, num_q, dim_v, dim_s, hidden_size):\n",
    "        super().__init__()\n",
    "        self.num_q = num_q\n",
    "        self.dim_v = dim_v\n",
    "        self.dim_s = dim_s\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.x_emb = Embedding(self.num_q * 2, self.dim_v)\n",
    "        self.knowledge_encoder = LSTM(self.dim_v, self.dim_v, batch_first=True)\n",
    "        self.out_layer = Linear(self.dim_v, self.dim_s)\n",
    "        self.dropout_layer = Dropout()\n",
    "\n",
    "        self.q_emb = Embedding(self.num_q, self.dim_v)\n",
    "        self.skill_encoder = Sequential(\n",
    "            Linear(self.dim_v, self.hidden_size),\n",
    "            ReLU(),\n",
    "            Linear(self.hidden_size, self.dim_v),\n",
    "            ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, q, r, qry):\n",
    "        # Knowledge State Encoding\n",
    "        x = q + self.num_q * r\n",
    "        x = self.x_emb(x)\n",
    "        h, _ = self.knowledge_encoder(x)\n",
    "        ks = self.out_layer(h)\n",
    "        ks = self.dropout_layer(ks)\n",
    "\n",
    "        # Skill Encoding\n",
    "        e = self.q_emb(qry)\n",
    "        o = self.skill_encoder(e)\n",
    "        s = o / torch.norm(o, p=2)\n",
    "\n",
    "        p = torch.sigmoid((ks * s).sum(-1))\n",
    "\n",
    "        return p\n",
    "\n",
    "    def train_model(\n",
    "        self, train_loader, test_loader, num_epochs, opt, ckpt_path\n",
    "    ):\n",
    "        '''\n",
    "            Args:\n",
    "                train_loader: the PyTorch DataLoader instance for training\n",
    "                test_loader: the PyTorch DataLoader instance for test\n",
    "                num_epochs: the number of epochs\n",
    "                opt: the optimization to train this model\n",
    "                ckpt_path: the path to save this model's parameters\n",
    "        '''\n",
    "        aucs = []\n",
    "        loss_means = []\n",
    "\n",
    "        max_auc = 0\n",
    "\n",
    "        for i in range(1, num_epochs + 1):\n",
    "            loss_mean = []\n",
    "\n",
    "            for data in train_loader:\n",
    "                q, r, qshft, rshft, m = data\n",
    "\n",
    "                self.train()\n",
    "\n",
    "                p = self(q.long(), r.long(), qshft.long())\n",
    "                p = torch.masked_select(p, m)\n",
    "                t = torch.masked_select(rshft, m)\n",
    "\n",
    "                opt.zero_grad()\n",
    "                loss = binary_cross_entropy(p, t)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                loss_mean.append(loss.detach().cpu().numpy())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data in test_loader:\n",
    "                    q, r, qshft, rshft, m = data\n",
    "\n",
    "                    self.eval()\n",
    "\n",
    "                    p = self(q.long(), r.long(), qshft.long())\n",
    "                    p = torch.masked_select(p, m).detach().cpu()\n",
    "                    t = torch.masked_select(rshft, m).detach().cpu()\n",
    "\n",
    "                    auc = metrics.roc_auc_score(\n",
    "                        y_true=t.numpy(), y_score=p.numpy()\n",
    "                    )\n",
    "\n",
    "                    loss_mean = np.mean(loss_mean)\n",
    "\n",
    "                    print(\n",
    "                        \"Epoch: {},   AUC: {},   Loss Mean: {}\"\n",
    "                        .format(i, auc, loss_mean)\n",
    "                    )\n",
    "\n",
    "                    if auc > max_auc:\n",
    "                        torch.save(\n",
    "                            self.state_dict(),\n",
    "                            os.path.join(\n",
    "                                ckpt_path, \"model.ckpt\"\n",
    "                            )\n",
    "                        )\n",
    "                        max_auc = auc\n",
    "\n",
    "                    aucs.append(auc)\n",
    "                    loss_means.append(loss_mean)\n",
    "\n",
    "        return aucs, loss_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01727f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edu_ktm",
   "language": "python",
   "name": "edu_ktm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
